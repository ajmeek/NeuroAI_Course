
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Microlearning: Credit assignment and local learning rules in artificial and neural systems &#8212; Neuromatch Academy: NeuroAI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'projects/project-notebooks/Microlearning';</script>
    <link rel="shortcut icon" href="../../_static/ai-logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Comparing networks: characterizing computational similarity in task-trained recurrent neural networks" href="ComparingNetworks.html" />
    <link rel="prev" title="Macrocircuits: leveraging neural architectural priors and modularity in embodied agents" href="Macrocircuits.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../../tutorials/intro.html">

  
  
  
  
  
  
  

  
    <img src="../../_static/ai-logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../_static/ai-logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/Schedule/schedule_intro.html">
                        Schedule
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/TechnicalHelp/tech_intro.html">
                        Technical Help
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/TechnicalHelp/Links_Policy.html">
                        Quick links and policies
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../prereqs/NeuroAI.html">
                        Prerequisites and preparatory materials for NeuroAI course
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W1D1_Generalization/chapter_title.html">
                        Generalization (W1D1)
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W1D2_ComparingTasks/chapter_title.html">
                        Comparing Tasks (W1D2)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/chapter_title.html">
                        Comparing Artificial And Biological Networks (W1D3)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W1D5_Microcircuits/chapter_title.html">
                        Microcircuits (W1D5)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W2D1_Macrocircuits/chapter_title.html">
                        Macrocircuits (W2D1)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W2D2_NeuroSymbolicStructures/chapter_title.html">
                        Neuro Symbolic Structures (W2D2)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W2D3_Microlearning/chapter_title.html">
                        Microlearning (W2D3)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W2D4_Macrolearning/chapter_title.html">
                        Macrolearning (W2D4)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W2D5_Mysteries/chapter_title.html">
                        Mysteries (W2D5)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../README.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../docs/project_guidance.html">
                        Daily guide for projects
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../docs/datasets_overview.html">
                        Project materials
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../professional_development/README.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../professional_development/impact_talks.html">
                        Impact Talks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../professional_development/mentorship_program.html">
                        Mentorship Program
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../professional_development/career_features.html">
                        Career Features
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../professional_development/career_panels.html">
                        Career Panels
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/Schedule/schedule_intro.html">
                        Schedule
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/TechnicalHelp/tech_intro.html">
                        Technical Help
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/TechnicalHelp/Links_Policy.html">
                        Quick links and policies
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../prereqs/NeuroAI.html">
                        Prerequisites and preparatory materials for NeuroAI course
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W1D1_Generalization/chapter_title.html">
                        Generalization (W1D1)
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W1D2_ComparingTasks/chapter_title.html">
                        Comparing Tasks (W1D2)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/chapter_title.html">
                        Comparing Artificial And Biological Networks (W1D3)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W1D5_Microcircuits/chapter_title.html">
                        Microcircuits (W1D5)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W2D1_Macrocircuits/chapter_title.html">
                        Macrocircuits (W2D1)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W2D2_NeuroSymbolicStructures/chapter_title.html">
                        Neuro Symbolic Structures (W2D2)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W2D3_Microlearning/chapter_title.html">
                        Microlearning (W2D3)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W2D4_Macrolearning/chapter_title.html">
                        Macrolearning (W2D4)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../tutorials/W2D5_Mysteries/chapter_title.html">
                        Mysteries (W2D5)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../README.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../docs/project_guidance.html">
                        Daily guide for projects
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../docs/datasets_overview.html">
                        Project materials
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../professional_development/README.html">
                        Introduction
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../professional_development/impact_talks.html">
                        Impact Talks
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../professional_development/mentorship_program.html">
                        Mentorship Program
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../professional_development/career_features.html">
                        Career Features
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../professional_development/career_panels.html">
                        Career Panels
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
  


<a class="navbar-brand logo" href="../../tutorials/intro.html">

  
  
  
  
  
  
  

  
    <img src="../../_static/ai-logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../_static/ai-logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    </div>
    <div class="sidebar-start-items__item">
<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../tutorials/intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/Schedule/schedule_intro.html">Schedule</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/Schedule/daily_schedules.html">General schedule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/Schedule/shared_calendars.html">Shared calendars</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/Schedule/timezone_widget.html">Timezone widget</a></li>
</ul>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/TechnicalHelp/tech_intro.html">Technical Help</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../tutorials/TechnicalHelp/Jupyterbook.html">Using jupyterbook</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/TechnicalHelp/Tutorial_colab.html">Using Google Colab</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../tutorials/TechnicalHelp/Tutorial_kaggle.html">Using Kaggle</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/TechnicalHelp/Discord.html">Using discord</a></li>
</ul>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/TechnicalHelp/Links_Policy.html">Quick links and policies</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../prereqs/NeuroAI.html">Prerequisites and preparatory materials for NeuroAI course</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/W1D1_Generalization/chapter_title.html">Generalization (W1D1)</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D1_Generalization/student/W1D1_Intro.html">W1D1 Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D1_Generalization/student/W1D1_Tutorial1.html">Tutorial 1: Generalization in AI</a></li>







<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D1_Generalization/student/W1D1_Tutorial2.html">Tutorial 2: Generalization in Neuroscience</a></li>







<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D1_Generalization/student/W1D1_Tutorial3.html">Tutorial 3: Generalization in Cognitive Science</a></li>


<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D1_Generalization/student/W1D1_Outro.html">Outro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D1_Generalization/student/W1D1_DaySummary.html">Day Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/W1D2_ComparingTasks/chapter_title.html">Comparing Tasks (W1D2)</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D2_ComparingTasks/student/W1D2_Intro.html">W1D2 Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D2_ComparingTasks/student/W1D2_Tutorial1.html"><strong>Tutorial 1:Task definition, application, relations and impacts on generalization</strong></a></li>








<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D2_ComparingTasks/student/W1D2_Tutorial2.html">Tutorial 2: Contrastive learning for object recognition</a></li>


<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D2_ComparingTasks/student/W1D2_Tutorial3.html">Tutorial 3: Reinforcement learning across temporal scales</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D2_ComparingTasks/student/W1D2_Outro.html">W1D2 Outro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D2_ComparingTasks/student/W1D2_DaySummary.html">W1D1 Day Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/chapter_title.html">Comparing Artificial And Biological Networks (W1D3)</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial1.html">Tutorial 1: Generalization and representational geometry</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial2.html">Tutorial 2: Computation as transformation of representational geometries</a></li>




<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial3.html">Tutorial 3: Representational geometry &amp; noise</a></li>







<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D3_ComparingArtificialAndBiologicalNetworks/student/W1D3_Tutorial4.html">Tutorial 4: Statistical inference on representational geometries</a></li>






</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Architectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/W1D5_Microcircuits/chapter_title.html">Microcircuits (W1D5)</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D5_Microcircuits/student/W1D5_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D5_Microcircuits/student/W1D5_Tutorial1.html">Tutorial 1: Sparsity and Sparse Coding</a></li>









<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D5_Microcircuits/student/W1D5_Tutorial2.html">Tutorial 2: Normalization</a></li>






<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D5_Microcircuits/student/W1D5_Tutorial3.html">Tutorial 3: Attention</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D5_Microcircuits/student/W1D5_Outro.html">Outro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W1D5_Microcircuits/student/W1D5_DaySummary.html">Day Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/W2D1_Macrocircuits/chapter_title.html">Macrocircuits (W2D1)</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D1_Macrocircuits/student/W2D1_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D1_Macrocircuits/student/W2D1_Tutorial1.html">Tutorial 1: Depth vs Width</a></li>






<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D1_Macrocircuits/student/W2D1_Outro.html">Outro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D1_Macrocircuits/further_reading.html">Suggested further readings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D1_Macrocircuits/student/W2D1_DaySummary.html">Day Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/W2D2_NeuroSymbolicStructures/chapter_title.html">Neuro Symbolic Structures (W2D2)</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D2_NeuroSymbolicStructures/student/W2D2_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D2_NeuroSymbolicStructures/student/W2D2_Tutorial1.html">Tutorial 1: Basic operations of vector symbolic algebra</a></li>








<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D2_NeuroSymbolicStructures/student/W2D2_Tutorial2.html">Tutorial 2: Learning from structures</a></li>






<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D2_NeuroSymbolicStructures/student/W2D2_Tutorial3.html">Tutorial 3: Generalizing representations in continuous space</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D2_NeuroSymbolicStructures/student/W2D2_Outro.html">Outro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D2_NeuroSymbolicStructures/further_reading.html">Suggested further readings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D2_NeuroSymbolicStructures/student/W2D2_DaySummary.html">Day Summary</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/W2D3_Microlearning/chapter_title.html">Microlearning (W2D3)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D3_Microlearning/student/W2D3_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D3_Microlearning/student/W2D3_Tutorial1.html">Tutorial 1: Microlearning</a></li>









<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D3_Microlearning/student/W2D3_Outro.html">Outro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D3_Microlearning/further_reading.html">Suggested further readings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D3_Microlearning/student/W2D3_DaySummary.html">Day Summary</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/W2D4_Macrolearning/chapter_title.html">Macrolearning (W2D4)</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D4_Macrolearning/student/W2D4_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D4_Macrolearning/student/W2D4_Tutorial1.html">Tutorial 1: The problem of changing data distributions</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D4_Macrolearning/student/W2D4_Tutorial2.html">Tutorial 2: Continual learning</a></li>





<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D4_Macrolearning/student/W2D4_Tutorial3.html">Tutorial 3: Meta-learning</a></li>






<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D4_Macrolearning/student/W2D4_Tutorial4.html">Tutorial 4: Biological meta reinforcement learning</a></li>








<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D4_Macrolearning/student/W2D4_Tutorial5.html">Tutorial 5: Replay</a></li>







<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D4_Macrolearning/student/W2D4_Outro.html">Outro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D4_Macrolearning/further_reading.html">Suggested further readings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D4_Macrolearning/student/W2D4_DaySummary.html">Day Summary</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Mysteries</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials/W2D5_Mysteries/chapter_title.html">Mysteries (W2D5)</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D5_Mysteries/student/W2D5_Intro.html">Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D5_Mysteries/student/W2D5_Tutorial1.html">Tutorial 1: Consciousness</a></li>




<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D5_Mysteries/student/W2D5_Tutorial2.html">Tutorial 2: Ethics</a></li>








<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D5_Mysteries/student/W2D5_Outro.html">Outro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D5_Mysteries/further_reading.html">Suggested further readings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tutorials/W2D5_Mysteries/student/W2D5_DaySummary.html">Day Summary</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Project Booklet</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../README.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../docs/project_guidance.html">Daily guide for projects</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../docs/datasets_overview.html">Project materials</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Macrocircuits.html">Macrocircuits: leveraging neural architectural priors and modularity in embodied agents</a></li>






<li class="toctree-l2 current active"><a class="current reference internal" href="#">Microlearning</a></li>
<li class="toctree-l2"><a class="reference internal" href="ComparingNetworks.html">Comparing networks: characterizing computational similarity in task-trained recurrent neural networks</a></li>


</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Professional Development</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../professional_development/README.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../professional_development/impact_talks.html">Impact Talks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../professional_development/mentorship_program.html">Professional developemnt</a></li>



<li class="toctree-l1"><a class="reference internal" href="../professional_development/career_features.html">Career Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../professional_development/career_panels.html">Career Panels</a></li>
</ul>

    </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        <label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" data-toggle="tooltip" data-placement="right" title="Toggle primary sidebar">
            <span class="fa-solid fa-bars"></span>
        </label>
    </div>
    <div class="header-article__right">


<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
  </ul>
</div>

<button onclick="toggleFullScreen()"
  class="btn btn-sm"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<div class="dropdown dropdown-repository-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://github.com/neuromatch/NeuroAI_Course" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">repository</span>
</a>
</a>
      
      <li><a href="https://github.com/neuromatch/NeuroAI_Course/issues/new?title=Issue%20on%20page%20%2Fprojects/project-notebooks/Microlearning.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">open issue</span>
</a>
</a>
      
  </ul>
</div>



<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="../../_sources/projects/project-notebooks/Microlearning.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</a>
      
      <li>
<button onclick="printPdf(this)"
  class="btn btn-sm dropdown-item"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</a>
      
  </ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary" data-toggle="tooltip" data-placement="left" title="Toggle secondary sidebar">
            <span class="fa-solid fa-list"></span>
        </label>
    </div>
</div>
            </div>
            
            

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Microlearning: Credit assignment and local learning rules in artificial and neural systems</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-1-initial-setup">
   Section 1: Initial Setup
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#download-mnist-dataset">
     1.1 Download MNIST dataset
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#explore-the-dataset">
       1.2 Explore the dataset
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-2-training-a-basic-model">
   Section 2: Training a basic model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#defining-a-basic-model">
     2.1 Defining a basic model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initializing-the-model">
     2.2 Initializing the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#defining-and-initializing-an-optimizer">
     2.3 Defining and initializing an optimizer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-a-basic-model">
     2.4 Training a basic model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-3-inspecting-a-model-s-performance">
   Section 3. Inspecting a model’s performance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-and-accuracy-across-learning">
     3.1 Loss and accuracy across learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#final-accuracy-per-class">
     3.2 Final accuracy per class
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classified-example-images">
     3.3 Classified example images
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weights-before-and-after-learning">
     3.4 Weights before and after learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-4-implementing-a-biologically-plausible-learning-rule">
   Section 4. Implementing a biologically plausible learning rule.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hebbian-learning">
     4.1 Hebbian learning
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#preventing-runaway-potentiation">
       4.1.1 Preventing runaway potentiation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-learning-rules-in-torch">
     4.2 Implementing learning rules in
     <code class="docutils literal notranslate">
      <span class="pre">
       torch
      </span>
     </code>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-a-custom-autograd-function">
       4.2.1 Defining a custom autograd function
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-the-forward-method">
       4.2.2 Defining the
       <code class="docutils literal notranslate">
        <span class="pre">
         forward()
        </span>
       </code>
       method
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-the-backward-method">
       4.2.3 Defining the
       <code class="docutils literal notranslate">
        <span class="pre">
         backward()
        </span>
       </code>
       method
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-a-hebbianmultilayerperceptron-class">
       4.2.4 Defining a
       <code class="docutils literal notranslate">
        <span class="pre">
         HebbianMultiLayerPerceptron
        </span>
       </code>
       class
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-a-model-with-hebbian-learning">
     4.3 Training a model with Hebbian learning
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#simplifying-the-task-to-a-2-class-task">
       4.3.1 Simplifying the task to a 2-class task
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-on-a-2-class-task">
       4.3.2 Training on a 2-class task
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-with-targets">
       4.3.3 Training with targets
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#increasing-task-difficulty">
       4.3.4 Increasing task difficulty
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#combining-hebbian-learning-and-error-backpropagation">
       4.3.5 Combining Hebbian learning and error backpropagation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-4-4-computing-the-variance-and-bias-of-a-model-s-gradients">
     Section 4.4. Computing the variance and bias of a model’s gradients.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variance">
     Variance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias">
     Bias
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#estimating-gradient-variance-using-snr">
       4.4.1 Estimating gradient variance using SNR
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#estimating-the-gradient-bias-with-respect-to-error-backpropagation-using-the-cosine-similarity">
       4.4.2 Estimating the gradient bias with respect to error backpropagation using the Cosine similarity.
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-5-implementing-additional-learning-rules">
   Section 5. Implementing additional learning rules.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-6-tips-suggestions">
   Section 6. Tips &amp; Suggestions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additional-references">
   Additional References
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#we-hope-you-enjoy-working-on-your-project-and-through-the-process-make-some-interesting-discoveries-about-the-challenges-and-potentials-of-biologically-plausible-learning">
     ⭐ We hope you enjoy working on your project and, through the process, make some interesting discoveries about the challenges and potentials of biologically plausible learning! ⭐
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>

            <article class="bd-article" role="main">
              
  <p><a href="https://colab.research.google.com/github/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/Microlearning.ipynb" target="_parent"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/Microlearning.ipynb" target="_parent"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg" /></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="microlearning-credit-assignment-and-local-learning-rules-in-artificial-and-neural-systems">
<h1>Microlearning: Credit assignment and local learning rules in artificial and neural systems<a class="headerlink" href="#microlearning-credit-assignment-and-local-learning-rules-in-artificial-and-neural-systems" title="Permalink to this heading">#</a></h1>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Colleen J. Gillon &amp; Klara Kaleb</p>
<p><strong>Content reviewers:</strong> Colleen J. Gillon, Klara Kaleb, Eva Dyer</p>
<p><strong>Production editors:</strong> Konstantine Tsafatinos, Ella Batty, Spiros Chavlis, Samuele Bolotta, Hlib Solodzhuk</p>
<hr class="docutils" />
<p><strong>Background:</strong> To learn effectively, our brain must coordinate synaptic updates across its network of neurons. The question of how the brain does this is called the credit assignment problem. Deep neural networks are a leading model of learning in the brain, and are typically trained using gradient descent via backpropagation. However, backpropagation is widely agreed to be biologically implausible. Therefore, to understand how the brain solves the credit assignment problem, we must find learning rules that are both biologically plausible and effective for learning. In this project, we will explore more biologically plausible learning rules proposed as alternatives to backpropagation, compare them to error backpropagation, and test whether we can infer what type of learning rule the brain might be using.</p>
<p><strong>Project setup:</strong> This project builds on a basic feedforward network trained to classify MNIST images (Q1). We then implement biologically plausible rules, compute performance and learning-related metrics (Q2-Q4 &amp; Q7), to then evaluate (1) how consistent and learning-rule specific the metrics are (Q5, Q8-9), or (2) how these rules fare in more complex learning scenarios (Q6, Q10).</p>
<p><strong>Relevant references:</strong></p>
<ul class="simple">
<li><p>Course materials from Neuromatch’s NeuroAI day on Microlearning</p></li>
<li><p>Review discussing backpropagation in the brain: <a class="reference external" href="https://www.nature.com/articles/s41583-020-0277-3">Lillicrap et al., 2020, Nature Reviews Neuroscience</a></p></li>
<li><p>Study on the reliability of different metrics for inferring learning rules: <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1ba922ac006a8e5f2b123684c2f4d65f-Paper.pdf">Nayebi et al., 2020, NeurIPS</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Project Background</span>

<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">widgets</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">IFrame</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>


<span class="k">class</span> <span class="nc">PlayVideo</span><span class="p">(</span><span class="n">IFrame</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">id</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">page</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">id</span> <span class="o">=</span> <span class="nb">id</span>
    <span class="k">if</span> <span class="n">source</span> <span class="o">==</span> <span class="s1">&#39;Bilibili&#39;</span><span class="p">:</span>
      <span class="n">src</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;https://player.bilibili.com/player.html?bvid=</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s1">&amp;page=</span><span class="si">{</span><span class="n">page</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="k">elif</span> <span class="n">source</span> <span class="o">==</span> <span class="s1">&#39;Osf&#39;</span><span class="p">:</span>
      <span class="n">src</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;https://mfr.ca-1.osf.io/render?url=https://osf.io/download/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s1">/?direct%26mode=render&#39;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PlayVideo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">display_videos</span><span class="p">(</span><span class="n">video_ids</span><span class="p">,</span> <span class="n">W</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="n">tab_contents</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">video_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">video_ids</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">Output</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">out</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">video_ids</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Youtube&#39;</span><span class="p">:</span>
        <span class="n">video</span> <span class="o">=</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">video_ids</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">width</span><span class="o">=</span><span class="n">W</span><span class="p">,</span>
                             <span class="n">height</span><span class="o">=</span><span class="n">H</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="n">fs</span><span class="p">,</span> <span class="n">rel</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Video available at https://youtube.com/watch?v=</span><span class="si">{</span><span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">video</span> <span class="o">=</span> <span class="n">PlayVideo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">video_ids</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">source</span><span class="o">=</span><span class="n">video_ids</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">width</span><span class="o">=</span><span class="n">W</span><span class="p">,</span>
                          <span class="n">height</span><span class="o">=</span><span class="n">H</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="n">fs</span><span class="p">,</span> <span class="n">autoplay</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">video_ids</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Bilibili&#39;</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Video available at https://www.bilibili.com/video/</span><span class="si">{</span><span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">video_ids</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Osf&#39;</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Video available at https://osf.io/</span><span class="si">{</span><span class="n">video</span><span class="o">.</span><span class="n">id</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
      <span class="n">display</span><span class="p">(</span><span class="n">video</span><span class="p">)</span>
    <span class="n">tab_contents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tab_contents</span>


<span class="c1"># video_ids = [(&#39;Youtube&#39;, &#39;&lt;video_id_1&gt;&#39;)]</span>
<span class="c1"># tab_contents = display_videos(video_ids, W=854, H=480)</span>
<span class="c1"># tabs = widgets.Tab()</span>
<span class="c1"># tabs.children = tab_contents</span>
<span class="c1"># for i in range(len(tab_contents)):</span>
<span class="c1">#   tabs.set_title(i, video_ids[i][0])</span>
<span class="c1"># display(tabs)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Project slides</span>

<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">IFrame</span>
<span class="n">link_id</span> <span class="o">=</span> <span class="s2">&quot;fjaqp&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;If you want to download the slides: https://osf.io/download/</span><span class="si">{</span><span class="n">link_id</span><span class="si">}</span><span class="s2">/&quot;</span><span class="p">)</span>
<span class="n">IFrame</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;https://mfr.ca-1.osf.io/render?url=https://osf.io/download/</span><span class="si">{</span><span class="n">link_id</span><span class="si">}</span><span class="s2">/?direct%26mode=render&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>If you want to download the slides: https://osf.io/download/fjaqp/
</pre></div>
</div>
<div class="output text_html">
        <iframe
            width="854"
            height="480"
            src="https://mfr.ca-1.osf.io/render?url=https://osf.io/download/fjaqp/?direct%26mode=render"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Project Template</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/MicrolearningProjectTemplate.svg?raw=true&quot;</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><img src="https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/MicrolearningProjectTemplate.svg?raw=true"/></div></div>
</div>
<section id="section-1-initial-setup">
<h2>Section 1: Initial Setup<a class="headerlink" href="#section-1-initial-setup" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Importing dependencies</span>

<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">SVG</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure settings</span>

<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>       <span class="c1"># interactive display</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/ClimateMatchAcademy/course-content/main/cma.mplstyle&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="download-mnist-dataset">
<h3>1.1 Download MNIST dataset<a class="headerlink" href="#download-mnist-dataset" title="Permalink to this heading">#</a></h3>
<p>The first step is to download the <a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST</a> handwritten digits dataset [1], which you will be using in this project. It is provided as a training dataset (60,000 examples) and a test dataset (10,000 examples). We can split the training dataset to obtain a training (e.g., 80%) and a validation set (e.g., 20%). In addition, since the dataset is quite large, we also suggest keeping only half of each subset, as this will make training the models faster.</p>
<p>[1] Deng, L. (2012). The MNIST database of handwritten digit images for machine learning research. <em>IEEE Signal Processing Magazine</em>, 29(6), 141–142, https://ieeexplore.ieee.org/document/6296535.</p>
<p><strong>Note:</strong> The download process may try a few sources before succeeding. <code class="docutils literal notranslate"><span class="pre">HTTP</span> <span class="pre">Error</span> <span class="pre">503:</span> <span class="pre">Service</span> <span class="pre">Unavailable</span></code> errors can be ignored.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown `download_mnist()`: Function to download MNIST.</span>

<span class="k">def</span> <span class="nf">download_mnist</span><span class="p">(</span><span class="n">train_prop</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">keep_prop</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>

  <span class="n">valid_prop</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">train_prop</span>

  <span class="n">discard_prop</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">keep_prop</span>

  <span class="n">transform</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span>
      <span class="p">[</span><span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
      <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))]</span>
      <span class="p">)</span>

  <span class="n">full_train_set</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span>
      <span class="n">root</span><span class="o">=</span><span class="s2">&quot;./data/&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
      <span class="p">)</span>
  <span class="n">full_test_set</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span>
      <span class="n">root</span><span class="o">=</span><span class="s2">&quot;./data/&quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
      <span class="p">)</span>

  <span class="n">train_set</span><span class="p">,</span> <span class="n">valid_set</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span>
      <span class="n">full_train_set</span><span class="p">,</span>
      <span class="p">[</span><span class="n">train_prop</span> <span class="o">*</span> <span class="n">keep_prop</span><span class="p">,</span> <span class="n">valid_prop</span> <span class="o">*</span> <span class="n">keep_prop</span><span class="p">,</span> <span class="n">discard_prop</span><span class="p">]</span>
      <span class="p">)</span>
  <span class="n">test_set</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span>
      <span class="n">full_test_set</span><span class="p">,</span>
      <span class="p">[</span><span class="n">keep_prop</span><span class="p">,</span> <span class="n">discard_prop</span><span class="p">]</span>
      <span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of examples retained:&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span><span class="si">}</span><span class="s2"> (training)&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_set</span><span class="p">)</span><span class="si">}</span><span class="s2"> (validation)&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_set</span><span class="p">)</span><span class="si">}</span><span class="s2"> (test)&quot;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">train_set</span><span class="p">,</span> <span class="n">valid_set</span><span class="p">,</span> <span class="n">test_set</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_set</span><span class="p">,</span> <span class="n">valid_set</span><span class="p">,</span> <span class="n">test_set</span> <span class="o">=</span> <span class="n">download_mnist</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of examples retained:
  24001 (training)
  5999 (validation)
  5000 (test)
</pre></div>
</div>
</div>
</div>
<section id="explore-the-dataset">
<h4>1.2 Explore the dataset<a class="headerlink" href="#explore-the-dataset" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown To get started exploring the dataset, here are a few plotting functions:</span>

<span class="c1">#@markdown `get_plotting_color()`: Returns a color for the specific dataset, e.g. &quot;train&quot; or model index.</span>
<span class="k">def</span> <span class="nf">get_plotting_color</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">model_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">model_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">if</span> <span class="n">model_idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">dataset</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">:</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;#1F77B4&quot;</span> <span class="c1"># blue</span>
  <span class="k">elif</span> <span class="n">model_idx</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">dataset</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;#FF7F0E&quot;</span> <span class="c1"># orange</span>
  <span class="k">elif</span> <span class="n">model_idx</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">dataset</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span>
    <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;#2CA02C&quot;</span> <span class="c1"># green</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">model_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Colors only implemented for up to 3 models.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2"> dataset not recognized. Expected &#39;train&#39;, &#39;valid&#39; &quot;</span>
          <span class="s2">&quot;or &#39;test&#39;.&quot;</span>
          <span class="p">)</span>

  <span class="k">return</span> <span class="n">color</span>


<span class="c1">#@markdown `plot_examples(subset)`: Plot examples from the dataset organized by their predicted class</span>
<span class="c1">#@markdown (if a model is provided) or by their class label otherwise</span>
<span class="k">def</span> <span class="nf">plot_examples</span><span class="p">(</span><span class="n">subset</span><span class="p">,</span> <span class="n">num_examples_per_class</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">MLP</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Function for visualizing example images from the dataset, organized by their</span>
<span class="sd">  predicted class, if a model is provided, or by their class, otherwise.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - subset (torch dataset or torch dataset subset): dataset from which to</span>
<span class="sd">    visualized images.</span>
<span class="sd">  - num_examples_per_class (int, optional): number of examples to visualize per</span>
<span class="sd">    class</span>
<span class="sd">  - MLP (MultiLayerPerceptron or None, optional): model to use to retrieve the</span>
<span class="sd">    predicted class for each image. If MLP is None, images will be organized by</span>
<span class="sd">    their class label. Otherwise, images will be organized by their predicted</span>
<span class="sd">    class.</span>
<span class="sd">  - seed (int or None, optional): Seed to use to randomly sample images to</span>
<span class="sd">    visualize.</span>
<span class="sd">  - batch_size (int, optional): If MLP is not None, number of images to</span>
<span class="sd">    retrieve predicted class for at one time.</span>
<span class="sd">  - num_classes (int, optional): Number of classes in the data.</span>
<span class="sd">  - ax (plt subplot, optional): Axis on which to plot images. If None, a new</span>
<span class="sd">    axis will be created.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - ax (plt subplot): Axis on which images were plotted.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">MLP</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Class&quot;</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">MLP</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Predicted class&quot;</span>

  <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fig_wid</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">*</span> <span class="mf">0.6</span><span class="p">)</span>
    <span class="n">fig_hei</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_examples_per_class</span> <span class="o">*</span> <span class="mf">0.6</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">fig_wid</span><span class="p">,</span> <span class="n">fig_hei</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">generator</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
    <span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

  <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
      <span class="n">subset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span>
      <span class="p">)</span>

  <span class="n">plot_images</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="nb">list</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)}</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">MLP</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

      <span class="n">done</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
        <span class="n">num_to_add</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_examples_per_class</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">plot_images</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="k">if</span> <span class="n">num_to_add</span><span class="p">:</span>
          <span class="n">add_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
          <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">add_images</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">add_i</span> <span class="ow">in</span> <span class="n">add_images</span><span class="p">[:</span> <span class="n">num_to_add</span><span class="p">]:</span>
              <span class="n">plot_images</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">add_i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
          <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">plot_images</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">!=</span> <span class="n">num_examples_per_class</span><span class="p">:</span>
            <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

      <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="k">break</span>

  <span class="n">hei</span><span class="p">,</span> <span class="n">wid</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">final_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">num_examples_per_class</span> <span class="o">*</span> <span class="n">hei</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">*</span> <span class="n">wid</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">images</span> <span class="ow">in</span> <span class="n">plot_images</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
      <span class="n">final_image</span><span class="p">[:</span> <span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">)</span> <span class="o">*</span> <span class="n">hei</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">wid</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">wid</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">final_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">wid</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)])</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Examples per </span><span class="si">{</span><span class="n">xlabel</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">ax</span>

<span class="c1">#@markdown `plot_class_distribution(train_set)`: Plots the distribution of classes in each set (train, validation, test).</span>
<span class="k">def</span> <span class="nf">plot_class_distribution</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">valid_set</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">test_set</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Function for plotting the number of examples per class in each subset.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - train_set (torch dataset or torch dataset subset): training dataset</span>
<span class="sd">  - valid_set (torch dataset or torch dataset subset, optional): validation</span>
<span class="sd">    dataset</span>
<span class="sd">  - test_set (torch dataset or torch dataset subset, optional): test</span>
<span class="sd">    dataset</span>
<span class="sd">  - num_classes (int, optional): Number of classes in the data.</span>
<span class="sd">  - ax (plt subplot, optional): Axis on which to plot images. If None, a new</span>
<span class="sd">    axis will be created.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - ax (plt subplot): Axis on which images were plotted.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

  <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>

  <span class="k">for</span> <span class="n">dataset_name</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="p">[</span>
      <span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">train_set</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">valid_set</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="n">test_set</span><span class="p">)</span>
      <span class="p">]:</span>
    <span class="k">if</span> <span class="n">dataset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">continue</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="s2">&quot;dataset&quot;</span><span class="p">):</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="n">dataset</span><span class="o">.</span><span class="n">indices</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">targets</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
        <span class="n">targets</span><span class="p">,</span>
        <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">get_plotting_color</span><span class="p">(</span><span class="n">dataset_name</span><span class="p">),</span>
        <span class="n">label</span><span class="o">=</span><span class="n">dataset_name</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">per_class</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_classes</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span>
        <span class="n">per_class</span><span class="p">,</span>
        <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">get_plotting_color</span><span class="p">(</span><span class="n">dataset_name</span><span class="p">),</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span>
        <span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">))</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Counts per class&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;center right&quot;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_examples</span><span class="p">(</span><span class="n">train_set</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/111a8528b7e285d4ca618dc1aec84cc496fb47ae1d4662f96bda8d73797872bd.png" src="../../_images/111a8528b7e285d4ca618dc1aec84cc496fb47ae1d4662f96bda8d73797872bd.png" />
</div>
</div>
</section>
</section>
</section>
<section id="section-2-training-a-basic-model">
<h2>Section 2: Training a basic model<a class="headerlink" href="#section-2-training-a-basic-model" title="Permalink to this heading">#</a></h2>
<section id="defining-a-basic-model">
<h3>2.1 Defining a basic model<a class="headerlink" href="#defining-a-basic-model" title="Permalink to this heading">#</a></h3>
<p>Next, we can define a basic model to train on this MNIST classification task.</p>
<p>First, it is helpful to define a <strong>few hyperparameters</strong> (<code class="docutils literal notranslate"><span class="pre">NUM_INPUTS</span></code> and <code class="docutils literal notranslate"><span class="pre">NUM_OUTPUTS</span></code>) to store the input size and output size the model needs to have for this task.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">MultiLayerPerceptron</span></code> class, provided here, initializes a <strong>multilayer perceptron (MLP) with one hidden layer</strong>. Feel free to expand or change the class, if you would like to use a different or more complex model, or add functionalities.</p>
<p>This class has several <strong>basic methods</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__init__(self)</span></code>: To initialize the model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">_set_activation(self)</span></code>: To set the specified activation function for the hidden layer. (The output layer has a softmax activation.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">forward(self,</span> <span class="pre">X)</span></code>: To define how activity is passed through the model.</p></li>
</ul>
<p>It also has additional methods that will be <strong>helpful later on</strong> for collecting metrics from the models:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">_store_initial_weights_biases(self)</span></code>: To store the initial weights and biases.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">forward_backprop(self,</span> <span class="pre">X)</span></code>: For when we will be comparing the gradients computed by alternative learning rules to the gradients computed by error backpropagation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">list_parameters(self)</span></code>: For convenience in retrieving a list of the model’s parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gather_gradient_dict(self)</span></code>: For gathering the gradients of the model’s parameters.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NUM_INPUTS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">train_set</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># size of an MNIST image</span>
<span class="n">NUM_OUTPUTS</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># number of MNIST classes</span>

<span class="k">class</span> <span class="nc">MultiLayerPerceptron</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Simple multilayer perceptron model class with one hidden layer.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
      <span class="bp">self</span><span class="p">,</span>
      <span class="n">num_inputs</span><span class="o">=</span><span class="n">NUM_INPUTS</span><span class="p">,</span>
      <span class="n">num_hidden</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
      <span class="n">num_outputs</span><span class="o">=</span><span class="n">NUM_OUTPUTS</span><span class="p">,</span>
      <span class="n">activation_type</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span>
      <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a multilayer perceptron with a single hidden layer.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    - num_inputs (int, optional): number of input units (i.e., image size)</span>
<span class="sd">    - num_hidden (int, optional): number of hidden units in the hidden layer</span>
<span class="sd">    - num_outputs (int, optional): number of output units (i.e., number of</span>
<span class="sd">      classes)</span>
<span class="sd">    - activation_type (str, optional): type of activation to use for the hidden</span>
<span class="sd">      layer (&#39;sigmoid&#39;, &#39;tanh&#39;, &#39;relu&#39; or &#39;linear&#39;)</span>
<span class="sd">    - bias (bool, optional): if True, each linear layer will have biases in</span>
<span class="sd">      addition to weights</span>
<span class="sd">    &quot;&quot;&quot;</span>


    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_inputs</span> <span class="o">=</span> <span class="n">num_inputs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden</span> <span class="o">=</span> <span class="n">num_hidden</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">=</span> <span class="n">num_outputs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation_type</span> <span class="o">=</span> <span class="n">activation_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>

    <span class="c1"># default weights (and biases, if applicable) initialization is used</span>
    <span class="c1"># see https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/linear.py</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lin1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lin2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_store_initial_weights_biases</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_set_activation</span><span class="p">()</span> <span class="c1"># activation on the hidden layer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># activation on the output layer</span>


  <span class="k">def</span> <span class="nf">_store_initial_weights_biases</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stores a copy of the network&#39;s initial weights and biases.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">init_lin1_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">init_lin2_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">init_lin1_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin1</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">init_lin2_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin2</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_set_activation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sets the activation function used for the hidden layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span> <span class="c1"># maps to [0, 1]</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;tanh&quot;</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span> <span class="c1"># maps to [-1, 1]</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span> <span class="c1"># maps to positive</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;identity&quot;</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span> <span class="c1"># maps to same</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">activation_type</span><span class="si">}</span><span class="s2"> activation type not recognized. Only &quot;</span>
          <span class="s2">&quot;&#39;sigmoid&#39;, &#39;relu&#39; and &#39;identity&#39; have been implemented so far.&quot;</span>
          <span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runs a forward pass through the network.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    - X (torch.Tensor): Batch of input images.</span>
<span class="sd">    - y (torch.Tensor, optional): Batch of targets. This variable is not used</span>
<span class="sd">      here. However, it may be needed for other learning rules, to it is</span>
<span class="sd">      included as an argument here for compatibility.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - y_pred (torch.Tensor): Predicted targets.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lin1</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_inputs</span><span class="p">)))</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lin2</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y_pred</span>

  <span class="k">def</span> <span class="nf">forward_backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identical to forward(). Should not be overwritten when creating new</span>
<span class="sd">    child classes to implement other learning rules, as this method is used</span>
<span class="sd">    to compare the gradients calculated with other learning rules to those</span>
<span class="sd">    calculated with backprop.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lin1</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_inputs</span><span class="p">)))</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lin2</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y_pred</span>


  <span class="k">def</span> <span class="nf">list_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a list of model names for a gradient dictionary.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - params_list (list): List of parameter names.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">params_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">layer_str</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;lin1&quot;</span><span class="p">,</span> <span class="s2">&quot;lin2&quot;</span><span class="p">]:</span>
      <span class="n">params_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">layer_str</span><span class="si">}</span><span class="s2">_weight&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
        <span class="n">params_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">layer_str</span><span class="si">}</span><span class="s2">_bias&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">params_list</span>


  <span class="k">def</span> <span class="nf">gather_gradient_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers a gradient dictionary for the model&#39;s parameters. Raises a</span>
<span class="sd">    runtime error if any parameters have no gradients.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - gradient_dict (dict): A dictionary of gradients for each parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">params_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">list_parameters</span><span class="p">()</span>

    <span class="n">gradient_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="n">params_list</span><span class="p">:</span>
      <span class="n">layer_str</span><span class="p">,</span> <span class="n">param_str</span> <span class="o">=</span> <span class="n">param_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)</span>
      <span class="n">layer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_str</span><span class="p">)</span>
      <span class="n">grad</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">param_str</span><span class="p">)</span><span class="o">.</span><span class="n">grad</span>
      <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;No gradient was computed&quot;</span><span class="p">)</span>
      <span class="n">gradient_dict</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">gradient_dict</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="initializing-the-model">
<h3>2.2 Initializing the model<a class="headerlink" href="#initializing-the-model" title="Permalink to this heading">#</a></h3>
<p>We can now <strong>initialize an MLP</strong>. Feel free to change the number of hidden units, change the activation function or include biases in the model.</p>
<p>Currently, the <code class="docutils literal notranslate"><span class="pre">&quot;sigmoid&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;TanH&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;ReLU&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;identity&quot;</span></code> activation functions are implemented, but you can add more by editing the <code class="docutils literal notranslate"><span class="pre">_set_activation(self)</span></code> method of the <code class="docutils literal notranslate"><span class="pre">MultiLayerPerceptron</span></code> class defined above.</p>
<p>We have set <code class="docutils literal notranslate"><span class="pre">BIAS=False</span></code> for simplicity.</p>
<p>We will also initialize the dataloaders. Feel free to select a different batch size (<code class="docutils literal notranslate"><span class="pre">BATCH_SIZE</span></code>) when training your models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model</span>
<span class="n">NUM_HIDDEN</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">ACTIVATION</span> <span class="o">=</span> <span class="s2">&quot;sigmoid&quot;</span> <span class="c1"># output constrained between 0 and 1</span>
<span class="n">BIAS</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">MLP</span> <span class="o">=</span> <span class="n">MultiLayerPerceptron</span><span class="p">(</span>
    <span class="n">num_hidden</span><span class="o">=</span><span class="n">NUM_HIDDEN</span><span class="p">,</span>
    <span class="n">activation_type</span><span class="o">=</span><span class="n">ACTIVATION</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="n">BIAS</span><span class="p">,</span>
    <span class="p">)</span>


<span class="c1"># Dataloaders</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">valid_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="defining-and-initializing-an-optimizer">
<h3>2.3 Defining and initializing an optimizer<a class="headerlink" href="#defining-and-initializing-an-optimizer" title="Permalink to this heading">#</a></h3>
<p>Here, we define a <strong>basic optimizer</strong> that updates the weights and biases of the model based on the gradients saved. This optimizer is equivalent to a simple <a class="reference external" href="https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD">Stochastic Gradient Descent optimizer</a> (<code class="docutils literal notranslate"><span class="pre">torch.optim.SGD()</span></code>) applied to mini-batch data.</p>
<p>It has two methods:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__init__(self)</span></code>: To initialize the optimizer. Any arguments passed after <code class="docutils literal notranslate"><span class="pre">params</span></code> should be added to the <code class="docutils literal notranslate"><span class="pre">defaults</span></code> dictionary, which is passed to the parents class. These arguments are then added to each parameter group’s dictionary, allowing them to be accessed in <code class="docutils literal notranslate"><span class="pre">step(self)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">step(self)</span></code>: Makes an update to the model parameters.</p></li>
</ul>
<p>This optimizer can be extended later, if needed, when implementing more complex learning rules.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BasicOptimizer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Simple optimizer class based on the SGD optimizer.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a basic optimizer object.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    - params (generator): Generator for torch model parameters.</span>
<span class="sd">    - lr (float, optional): Learning rate.</span>
<span class="sd">    - weight_decay (float, optional): Weight decay.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">lr</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid learning rate: </span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid weight_decay value: </span><span class="si">{</span><span class="n">weight_decay</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      Performs a single optimization step.</span>
<span class="sd">      &quot;&quot;&quot;</span>

      <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>

          <span class="c1"># only update parameters with gradients</span>
          <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

            <span class="c1"># apply weight decay to gradient, if applicable</span>
            <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
              <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">])</span>

            <span class="c1"># apply gradient-based update</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We can now <strong>initialize an optimizer</strong>. Feel free to change to learning rate (<code class="docutils literal notranslate"><span class="pre">LR</span></code>) that the optimizer is initialized with, or to add a weight decay.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LR</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">backprop_optimizer</span> <span class="o">=</span> <span class="n">BasicOptimizer</span><span class="p">(</span><span class="n">MLP</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-a-basic-model">
<h3>2.4 Training a basic model<a class="headerlink" href="#training-a-basic-model" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown `train_model(MLP, train_loader, valid_loader, optimizer)`: Main function.</span>
<span class="c1">#@markdown Trains the model across epochs. Aggregates loss and accuracy statistics</span>
<span class="c1">#@markdown from the training and validation datasets into a results dictionary which is returned.</span>
<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Train a model for several epochs.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - MLP (torch model): Model to train.</span>
<span class="sd">  - train_loader (torch dataloader): Dataloader to use to train the model.</span>
<span class="sd">  - valid_loader (torch dataloader): Dataloader to use to validate the model.</span>
<span class="sd">  - optimizer (torch optimizer): Optimizer to use to update the model.</span>
<span class="sd">  - num_epochs (int, optional): Number of epochs to train model.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - results_dict (dict): Dictionary storing results across epochs on training</span>
<span class="sd">    and validation data.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">results_dict</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s2">&quot;avg_train_losses&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
      <span class="s2">&quot;avg_valid_losses&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
      <span class="s2">&quot;avg_train_accuracies&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
      <span class="s2">&quot;avg_valid_accuracies&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(),</span>
  <span class="p">}</span>

  <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)):</span>
    <span class="n">no_train</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">e</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">False</span> <span class="c1"># to get a baseline</span>
    <span class="n">latest_epoch_results_dict</span> <span class="o">=</span> <span class="n">train_epoch</span><span class="p">(</span>
        <span class="n">MLP</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">no_train</span><span class="o">=</span><span class="n">no_train</span>
        <span class="p">)</span>

    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">latest_epoch_results_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">results_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">results_dict</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">results_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">latest_epoch_results_dict</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">results_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="c1"># copy latest</span>

  <span class="k">return</span> <span class="n">results_dict</span>


<span class="k">def</span> <span class="nf">train_epoch</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">no_train</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Train a model for one epoch.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - MLP (torch model): Model to train.</span>
<span class="sd">  - train_loader (torch dataloader): Dataloader to use to train the model.</span>
<span class="sd">  - valid_loader (torch dataloader): Dataloader to use to validate the model.</span>
<span class="sd">  - optimizer (torch optimizer): Optimizer to use to update the model.</span>
<span class="sd">  - no_train (bool, optional): If True, the model is not trained for the</span>
<span class="sd">    current epoch. Allows a baseline (chance) performance to be computed in the</span>
<span class="sd">    first epoch before training starts.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - epoch_results_dict (dict): Dictionary storing epoch results on training</span>
<span class="sd">    and validation data.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

  <span class="n">epoch_results_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">sub_str</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;correct_by_class&quot;</span><span class="p">,</span> <span class="s2">&quot;seen_by_class&quot;</span><span class="p">]:</span>
      <span class="n">epoch_results_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">sub_str</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
          <span class="n">i</span><span class="p">:</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MLP</span><span class="o">.</span><span class="n">num_outputs</span><span class="p">)</span>
          <span class="p">}</span>

  <span class="n">MLP</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
  <span class="n">train_losses</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">update_results_by_class_in_place</span><span class="p">(</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">epoch_results_dict</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="o">=</span><span class="n">MLP</span><span class="o">.</span><span class="n">num_outputs</span>
        <span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">no_train</span><span class="p">:</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

  <span class="n">num_items</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
  <span class="n">epoch_results_dict</span><span class="p">[</span><span class="s2">&quot;avg_train_losses&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">train_losses</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_items</span>
  <span class="n">epoch_results_dict</span><span class="p">[</span><span class="s2">&quot;avg_train_accuracies&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_items</span> <span class="o">*</span> <span class="mi">100</span>

  <span class="n">MLP</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
  <span class="n">valid_losses</span><span class="p">,</span> <span class="n">valid_acc</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">valid_loader</span><span class="p">:</span>
      <span class="n">y_pred</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
      <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
      <span class="n">valid_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
      <span class="n">valid_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
      <span class="n">update_results_by_class_in_place</span><span class="p">(</span>
          <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">epoch_results_dict</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span>
          <span class="p">)</span>

  <span class="n">num_items</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
  <span class="n">epoch_results_dict</span><span class="p">[</span><span class="s2">&quot;avg_valid_losses&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">valid_losses</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_items</span>
  <span class="n">epoch_results_dict</span><span class="p">[</span><span class="s2">&quot;avg_valid_accuracies&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">valid_acc</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_items</span> <span class="o">*</span> <span class="mi">100</span>

  <span class="k">return</span> <span class="n">epoch_results_dict</span>


<span class="k">def</span> <span class="nf">update_results_by_class_in_place</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">result_dict</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
                                     <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Updates results dictionary in place during a training epoch by adding data</span>
<span class="sd">  needed to compute the accuracies for each class.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - y (torch Tensor): target labels</span>
<span class="sd">  - y_pred (torch Tensor): predicted targets</span>
<span class="sd">  - result_dict (dict): Dictionary storing epoch results on training</span>
<span class="sd">    and validation data.</span>
<span class="sd">  - dataset (str, optional): Dataset for which results are being added.</span>
<span class="sd">  - num_classes (int, optional): Number of classes.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">correct_by_class</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="n">seen_by_class</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Number of predictions does not match number of targets.&quot;</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">result_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">_seen_by_class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">result_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">_seen_by_class&quot;</span><span class="p">][</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">idxs</span><span class="p">)</span>

    <span class="n">num_correct</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">idxs</span><span class="p">]))</span>
    <span class="n">result_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">_correct_by_class&quot;</span><span class="p">][</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">num_correct</span>
</pre></div>
</div>
</div>
</div>
<p>Once the model and optimizer have been initialized, we can now <strong>train our model</strong> using backpropagation for a few epochs, and collect the classification results dictionary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">MLP_results_dict</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">MLP</span><span class="p">,</span>
    <span class="n">train_loader</span><span class="p">,</span>
    <span class="n">valid_loader</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">backprop_optimizer</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<p><strong>Note:</strong> The training function does not use the <code class="docutils literal notranslate"><span class="pre">test_loader</span></code>. This additional dataloader can be used to evaluate the models if, for example, you need use the validation set for model selection.</p>
</section>
</section>
<section id="section-3-inspecting-a-model-s-performance">
<h2>Section 3. Inspecting a model’s performance<a class="headerlink" href="#section-3-inspecting-a-model-s-performance" title="Permalink to this heading">#</a></h2>
<p>The <strong>results dictionary</strong> (<code class="docutils literal notranslate"><span class="pre">MLP_results_dict</span></code>) returned by <code class="docutils literal notranslate"><span class="pre">train_model()</span></code> contains the following keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">avg_train_losses</span></code>: Average training loss per epoch.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">avg_valid_losses</span></code>: Average validation loss per epoch.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">avg_train_accuracies</span></code>: Average training accuracies per epoch.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">avg_valid_losses</span></code>: Average validation accuracies per epoch.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_correct_by_class</span></code>: Number of correctly classified training images for each class (last epoch only).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_seen_by_class</span></code>: Number of training images for each class (last epoch only).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">valid_correct_by_class</span></code>: Number of correctly classified validation images for each class (last epoch only).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">valid_seen_by_class</span></code>: Number of validation images for each class (last epoch only).</p></li>
</ul>
<p>Next, we can inspect our model’s performance by visualizing various <strong>metrics</strong>, e.g., classification loss, accuracy, accuracy by class, weights. A few example functions are provided for plotted various metrics collected across learning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown `plot_results(results_dict)`: Plots classification losses and</span>
<span class="c1">#@markdown accuracies across epochs for the training and validation sets.</span>
<span class="k">def</span> <span class="nf">plot_results</span><span class="p">(</span><span class="n">results_dict</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Function for plotting losses and accuracies across learning.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - results_dict (dict): Dictionary storing results across epochs on training</span>
<span class="sd">    and validation data.</span>
<span class="sd">  - num_classes (float, optional): Number of classes, used to calculate chance</span>
<span class="sd">    accuracy.</span>
<span class="sd">  - ax (plt subplot, optional): Axis on which to plot results. If None, a new</span>
<span class="sd">    axis will be created.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - ax (plt subplot): Axis on which results were plotted.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">))</span>

  <span class="n">loss_ax</span> <span class="o">=</span> <span class="n">ax</span>
  <span class="n">acc_ax</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="n">chance</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">/</span> <span class="n">num_classes</span>

  <span class="n">plotted</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="k">for</span> <span class="n">result_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;losses&quot;</span><span class="p">,</span> <span class="s2">&quot;accuracies&quot;</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">]:</span>
      <span class="n">key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;avg_</span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">result_type</span><span class="si">}</span><span class="s2">&quot;</span>
      <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">results_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">result_type</span> <span class="o">==</span> <span class="s2">&quot;losses&quot;</span><span class="p">:</span>
          <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;Loss&quot;</span>
          <span class="n">plot_ax</span> <span class="o">=</span> <span class="n">loss_ax</span>
          <span class="n">ls</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="n">result_type</span> <span class="o">==</span> <span class="s2">&quot;accuracies&quot;</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">acc_ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">acc_ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">twinx</span><span class="p">()</span>
            <span class="n">acc_ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[[</span><span class="s2">&quot;right&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">acc_ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">chance</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
            <span class="n">acc_ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">105</span><span class="p">)</span>
          <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;Accuracy (%)&quot;</span>
          <span class="n">plot_ax</span> <span class="o">=</span> <span class="n">acc_ax</span>
          <span class="n">ls</span> <span class="o">=</span> <span class="s2">&quot;dashed&quot;</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">result_type</span><span class="si">}</span><span class="s2"> result type not recognized.&quot;</span><span class="p">)</span>

        <span class="n">data</span> <span class="o">=</span> <span class="n">results_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="n">plot_ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">data</span><span class="p">,</span>
            <span class="n">ls</span><span class="o">=</span><span class="n">ls</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="n">get_plotting_color</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">plot_ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">)</span>
        <span class="n">plotted</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">if</span> <span class="n">plotted</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;center left&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))])</span>
    <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">ymin</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">ymin</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">ymax</span> <span class="o">-</span> <span class="n">ymin</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.05</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ymin</span> <span class="o">-</span> <span class="n">pad</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">+</span> <span class="n">pad</span><span class="p">)</span>

  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;No data found to plot.&quot;</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Performance across learning&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">ax</span>


<span class="c1">#@markdown `plot_scores_per_class(results_dict)`: Plots the classification</span>
<span class="c1">#@markdown accuracies by class for the training and validation sets (for the last epoch).</span>
<span class="k">def</span> <span class="nf">plot_scores_per_class</span><span class="p">(</span><span class="n">results_dict</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Function for plotting accuracy scores for each class.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - results_dict (dict): Dictionary storing results across epochs on training</span>
<span class="sd">    and validation data.</span>
<span class="sd">  - num_classes (int, optional): Number of classes in the data.</span>
<span class="sd">  - ax (plt subplot, optional): Axis on which to plot accuracies. If None, a new</span>
<span class="sd">    axis will be created.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - ax (plt subplot): Axis on which accuracies were plotted.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

  <span class="n">avgs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_prop_cycle</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span> <span class="c1"># reset color cycle</span>
  <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">]):</span>
    <span class="n">correct_by_class</span> <span class="o">=</span> <span class="n">results_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">_correct_by_class&quot;</span><span class="p">]</span>
    <span class="n">seen_by_class</span> <span class="o">=</span> <span class="n">results_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">_seen_by_class&quot;</span><span class="p">]</span>
    <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">total</span> <span class="ow">in</span> <span class="n">seen_by_class</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">total</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">correct_by_class</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span>

    <span class="n">avg_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;avg_</span><span class="si">{</span><span class="n">dataset</span><span class="si">}</span><span class="s2">_accuracies&quot;</span>
    <span class="k">if</span> <span class="n">avg_key</span> <span class="ow">in</span> <span class="n">results_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span>
          <span class="n">results_dict</span><span class="p">[</span><span class="n">avg_key</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
          <span class="n">color</span><span class="o">=</span><span class="n">get_plotting_color</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
          <span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span>
        <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">get_plotting_color</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        <span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">))</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy (%)&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Class scores&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">105</span><span class="p">)</span>

  <span class="n">chance</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">/</span> <span class="n">num_classes</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">chance</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">ax</span>


<span class="c1">#@markdown `plot_weights(MLP)`: Plots weights before and after training.</span>
<span class="k">def</span> <span class="nf">plot_weights</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">shared_colorbar</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Function for plotting model weights and biases before and after learning.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - MLP (torch model): Model for which to plot weights and biases.</span>
<span class="sd">  - shared_colorbar (bool, optional): If True, one colorbar is shared for all</span>
<span class="sd">      parameters.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - ax (plt subplot array): Axes on which weights and biases were plotted.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">param_names</span> <span class="o">=</span> <span class="n">MLP</span><span class="o">.</span><span class="n">list_parameters</span><span class="p">()</span>

  <span class="n">params_images</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
  <span class="n">pre_means</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
  <span class="n">post_means</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
  <span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
  <span class="k">for</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="n">param_names</span><span class="p">:</span>
    <span class="n">layer</span><span class="p">,</span> <span class="n">param_type</span> <span class="o">=</span> <span class="n">param_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)</span>
    <span class="n">init_params</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;init_</span><span class="si">{</span><span class="n">layer</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">param_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">separator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">init_params</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="n">last_params</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">layer</span><span class="p">),</span> <span class="n">param_type</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">diff_params</span> <span class="o">=</span> <span class="n">last_params</span> <span class="o">-</span> <span class="n">init_params</span>

    <span class="n">params_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">init_params</span><span class="p">,</span> <span class="n">separator</span><span class="p">,</span> <span class="n">last_params</span><span class="p">,</span> <span class="n">separator</span><span class="p">,</span> <span class="n">diff_params</span><span class="p">]</span>
        <span class="p">)</span>
    <span class="n">vmin</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">vmin</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmin</span><span class="p">(</span><span class="n">params_image</span><span class="p">))</span>
    <span class="n">vmax</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">vmax</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmax</span><span class="p">(</span><span class="n">params_image</span><span class="p">))</span>

    <span class="n">params_images</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">params_image</span>
    <span class="n">pre_means</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">init_params</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">post_means</span><span class="p">[</span><span class="n">param_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">last_params</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

  <span class="n">nrows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span>
  <span class="n">gridspec_kw</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
    <span class="n">gridspec_kw</span><span class="p">[</span><span class="s2">&quot;height_ratios&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">cbar_label</span> <span class="o">=</span> <span class="s2">&quot;Weight/bias values&quot;</span>
  <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
    <span class="n">gridspec_kw</span><span class="p">[</span><span class="s2">&quot;height_ratios&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
    <span class="n">cbar_label</span> <span class="o">=</span> <span class="s2">&quot;Weight values&quot;</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Expected 2 parameters (weights only) or &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;4 parameters (weights and biases), but found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">param_names</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

  <span class="k">if</span> <span class="n">shared_colorbar</span><span class="p">:</span>
    <span class="n">nrows</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">gridspec_kw</span><span class="p">[</span><span class="s2">&quot;height_ratios&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

  <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span>
      <span class="n">nrows</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">nrows</span> <span class="o">+</span> <span class="mi">3</span><span class="p">),</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="n">gridspec_kw</span>
      <span class="p">)</span>

  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">params_image</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params_images</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">layer</span><span class="p">,</span> <span class="n">param_type</span> <span class="o">=</span> <span class="n">param_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)</span>
    <span class="n">layer_str</span> <span class="o">=</span> <span class="s2">&quot;First&quot;</span> <span class="k">if</span> <span class="n">layer</span> <span class="o">==</span> <span class="s2">&quot;lin1&quot;</span> <span class="k">else</span> <span class="s2">&quot;Second&quot;</span>
    <span class="n">param_str</span> <span class="o">=</span> <span class="s2">&quot;weights&quot;</span> <span class="k">if</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;weight&quot;</span> <span class="k">else</span> <span class="s2">&quot;biases&quot;</span>

    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">layer_str</span><span class="si">}</span><span class="s2"> linear layer </span><span class="si">{</span><span class="n">param_str</span><span class="si">}</span><span class="s2"> (pre, post and diff)&quot;</span><span class="p">)</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">params_image</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">shared_colorbar</span><span class="p">:</span>
      <span class="n">cbar</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
      <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">pre_means</span><span class="p">[</span><span class="n">param_name</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;dotted&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
      <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">post_means</span><span class="p">[</span><span class="n">param_name</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span>
      <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Input dim.&quot;</span><span class="p">)</span>
      <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Output dim.&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">spines</span><span class="p">[[</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="s2">&quot;bottom&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>

  <span class="k">if</span> <span class="n">shared_colorbar</span><span class="p">:</span>
    <span class="n">cax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">cbar</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s2">&quot;horizontal&quot;</span><span class="p">,</span> <span class="n">location</span><span class="o">=</span><span class="s2">&quot;bottom&quot;</span><span class="p">)</span>
    <span class="n">cax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">cbar_label</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">axes</span>
</pre></div>
</div>
</div>
</div>
<section id="loss-and-accuracy-across-learning">
<h3>3.1 Loss and accuracy across learning<a class="headerlink" href="#loss-and-accuracy-across-learning" title="Permalink to this heading">#</a></h3>
<p>First, we can look at how the loss (full lines) and accuracy (dashed lines) evolve across learning. The model appears to <strong>learn quickly</strong>, achieving a performance on the validation dataset that is <strong>well above chance</strong> accuracy (black dashed line), even with only 5 training epochs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_results</span><span class="p">(</span><span class="n">MLP_results_dict</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9216ecfd9f5746fe86fbe8bc5afb963ae5b71365752a02650e39a90b2de0f51c.png" src="../../_images/9216ecfd9f5746fe86fbe8bc5afb963ae5b71365752a02650e39a90b2de0f51c.png" />
</div>
</div>
</section>
<section id="final-accuracy-per-class">
<h3>3.2 Final accuracy per class<a class="headerlink" href="#final-accuracy-per-class" title="Permalink to this heading">#</a></h3>
<p>We can also look at the <strong>accuracy breakdown</strong> per class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_scores_per_class</span><span class="p">(</span><span class="n">MLP_results_dict</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/0124009ff3c8a0aa10a065840accaa485ae47572685e5f995cba4e52d19e3e52.png" src="../../_images/0124009ff3c8a0aa10a065840accaa485ae47572685e5f995cba4e52d19e3e52.png" />
</div>
</div>
</section>
<section id="classified-example-images">
<h3>3.3 Classified example images<a class="headerlink" href="#classified-example-images" title="Permalink to this heading">#</a></h3>
<p>We can <strong>visualize</strong> examples of how the model has classified certain images (correctly or incorrectly).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_examples</span><span class="p">(</span><span class="n">valid_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">MLP</span><span class="o">=</span><span class="n">MLP</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/50d6ce571ffaf277147589740d605b78510935321be8d41518058a01ea3cb655.png" src="../../_images/50d6ce571ffaf277147589740d605b78510935321be8d41518058a01ea3cb655.png" />
</div>
</div>
</section>
<section id="weights-before-and-after-learning">
<h3>3.4 Weights before and after learning<a class="headerlink" href="#weights-before-and-after-learning" title="Permalink to this heading">#</a></h3>
<p>We can also observe how the weights changed through learning by visualizing the <strong>initial</strong> weights (top), the weights <strong>after</strong> learning (middle), and the <strong>difference</strong> between the two (bottom).</p>
<p>The average weights <strong>before</strong> learning (dashed line) and <strong>after</strong> learning (full line) are plotted on the colorbar for each layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_weights</span><span class="p">(</span><span class="n">MLP</span><span class="o">=</span><span class="n">MLP</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/d7092af310f85e991a57ba2c8d116fbdb8d785589ec863fa6ec822ffa3a67cd0.png" src="../../_images/d7092af310f85e991a57ba2c8d116fbdb8d785589ec863fa6ec822ffa3a67cd0.png" />
</div>
</div>
<p>❓ <strong>What other metrics might you collect or visualize to understand how the model is learning to perform this task?</strong></p>
<p><strong>Note:</strong> In this section, we have visualized a variety of metrics that can be used to evaluate and compare models. Later in the project, you may want to <strong>collect and record</strong> these metrics for each trained model, instead of just visualizing the them.</p>
</section>
</section>
<section id="section-4-implementing-a-biologically-plausible-learning-rule">
<h2>Section 4. Implementing a biologically plausible learning rule.<a class="headerlink" href="#section-4-implementing-a-biologically-plausible-learning-rule" title="Permalink to this heading">#</a></h2>
<section id="hebbian-learning">
<h3>4.1 Hebbian learning<a class="headerlink" href="#hebbian-learning" title="Permalink to this heading">#</a></h3>
<p>Now, it is time to implement a more biologically plausible learning rule, like <strong>Hebbian learning</strong>. This rule is famously associated with the phrase <em>“Neurons that fire together wire together.”</em></p>
<p>In Hebbian learning, the weight <span class="math notranslate nohighlight">\(w_{ij}\)</span> between pre-synaptic neuron <em>i</em> and post-synaptic neuron <em>j</em> is updated as follows:<br />
<span class="math notranslate nohighlight">\({\Delta}w_{ij} = {\eta}(a_{i} \cdot a_{j})\)</span>,</p>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\({\eta}\)</span> is the learning rate,</p></li>
<li><p><span class="math notranslate nohighlight">\(a_{i}\)</span> is the activation of the pre-synaptic neuron <em>i</em>, and</p></li>
<li><p><span class="math notranslate nohighlight">\(a_{j}\)</span> is the activation of the post-synaptic neuron <em>j</em>.</p></li>
</ul>
<p>This means that the <strong>weight update</strong> between two neurons is <strong>proportional to the correlation</strong> in their activity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown Hebbian learning schematic</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/HebbianLearning.jpeg?raw=true&quot;</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">url</span> <span class="o">=</span> <span class="n">url</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">400</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><img src="https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/HebbianLearning.jpeg?raw=true" height="400"/></div></div>
</div>
<p><strong><em>(Left)</em> Hebbian learning when only positive neural activity is allowed.</strong></p>
<p>Input neurons (top) are connected to output neurons (bottom). Active neurons are marked with + (active, light blue) and ++ (very active, dark blue). Connections marked with + will be weakly increased, and those marked with ++ will be strongly increased by Hebbian learning.</p>
<p><strong><em>(Right)</em> Hebbian learning when positive and negative neural activity is allowed.</strong><br />
Same as (Left), but negatively active neurons are marked with - (slightly active, light red) and – (very active, dark red). Connections marked with - will be weakly decreased, and those marked with – will be strongly decreased by Hebbian learning.</p>
<section id="preventing-runaway-potentiation">
<h4>4.1.1 Preventing runaway potentiation<a class="headerlink" href="#preventing-runaway-potentiation" title="Permalink to this heading">#</a></h4>
<p>Networks trained with Hebbian learning are typically implemented with neurons that only have <strong>positive activations</strong> <em>(Left)</em>, like neurons in the brain. This means that weights between neurons can only <strong>increase</strong>.</p>
<p>Allowing neurons to have both <strong>positive and negative</strong> activations <em>(Right)</em> would allow weights to also decrease with Hebbian learning. However, this is not typically done since <strong>negative neural activity</strong> (i.e., a negative firing rate) <strong>is not biologically plausible</strong>.</p>
<p>Instead, various techniques can be used to prevent <strong>runaway potentiation</strong> in Hebbian learning (i.e., weights increasing more and more, without limit). These include normalization techniques like <a class="reference external" href="http://www.scholarpedia.org/article/Oja_learning_rule">Oja’s rule</a>. In the example below, we take the approach of simply centering weight updates around 0.</p>
</section>
</section>
<section id="implementing-learning-rules-in-torch">
<h3>4.2 Implementing learning rules in <code class="docutils literal notranslate"><span class="pre">torch</span></code><a class="headerlink" href="#implementing-learning-rules-in-torch" title="Permalink to this heading">#</a></h3>
<section id="defining-a-custom-autograd-function">
<h4>4.2.1 Defining a custom autograd function<a class="headerlink" href="#defining-a-custom-autograd-function" title="Permalink to this heading">#</a></h4>
<p>One way to train a torch model using a learning rule other than backpropagation is to create a <strong>custom autograd function</strong>. With a custom autograd function, we can redefine the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> and <code class="docutils literal notranslate"><span class="pre">backward()</span></code> methods that <strong>pass</strong> activations forward through the network, and pass gradients backward through the network. Specifically, this allows us to specify <strong>how</strong> the gradients used to update the model should be calculated.</p>
<p>To implement Hebbian learning in the model, we will create a <strong>custom autograd function</strong> called <code class="docutils literal notranslate"><span class="pre">HebbianFunction</span></code>.</p>
</section>
<section id="defining-the-forward-method">
<h4>4.2.2 Defining the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method<a class="headerlink" href="#defining-the-forward-method" title="Permalink to this heading">#</a></h4>
<p>The forward method of an autograd function serves two purposes:</p>
<ol class="arabic simple">
<li><p><strong>Compute an output</strong> for the given input.</p></li>
<li><p>Gather and store <strong>all the information needed</strong> to compute weight updates during the <strong>backward</strong> pass.</p></li>
</ol>
<p>As explained above, to calculate the Hebbian weight change between neurons in two layers, we need the <strong>activity of the input</strong> neurons and the <strong>activity of the output</strong> neurons. So, the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method should receive <strong>input neuron activity</strong> as its input, and return <strong>output neuron activity</strong> as its output.</p>
<p>The inputs to <code class="docutils literal notranslate"><span class="pre">forward()</span></code> are therefore:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">context</span></code>: Passed implictly to <code class="docutils literal notranslate"><span class="pre">forward()</span></code>, and used to store any information needed to calculate gradients during the backward pass.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input</span></code>: The input neuron activity.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight</span></code>: The linear layer’s weights.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bias</span></code>: The linear layer’s biases (can be <code class="docutils literal notranslate"><span class="pre">None</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nonlinearity</span></code>: The nonlinearity function, as it will be needed to calculate the output neuron’s <strong>activity</strong>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">target</span></code>: As will be explained later, for Hebbian learning, it can be very useful to use the targets to training the last layer of the network, instead of true output activity. So here, if targets are passed to <code class="docutils literal notranslate"><span class="pre">forward()</span></code>, this is stored instead of the output.</p></li>
</ul>
<p>In the forward pass, output neuron activity is computed, and the following variables are saved for the backward pass: <code class="docutils literal notranslate"><span class="pre">input</span></code>, <code class="docutils literal notranslate"><span class="pre">weight</span></code>, <code class="docutils literal notranslate"><span class="pre">bias</span></code> and <code class="docutils literal notranslate"><span class="pre">output_for_update</span></code> (i.e., the computed output neuron activity or the <code class="docutils literal notranslate"><span class="pre">target</span></code>, if it’s provided, averaged across the batch).</p>
</section>
<section id="defining-the-backward-method">
<h4>4.2.3 Defining the <code class="docutils literal notranslate"><span class="pre">backward()</span></code> method<a class="headerlink" href="#defining-the-backward-method" title="Permalink to this heading">#</a></h4>
<p>The <strong><code class="docutils literal notranslate"><span class="pre">backward()</span></code></strong> method of an autograd function computes and returns gradients using only two input variables:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">context</span></code>: In which information was stored during the forward pass.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">grad_output</span></code>: The <code class="docutils literal notranslate"><span class="pre">grad_input</span></code> from the downstream layer (since gradients are computed <strong>backwards</strong> through the network.</p></li>
</ul>
<p>Here, for Hebbian learning, we do not use a backpropagated gradient. For this reason, <code class="docutils literal notranslate"><span class="pre">grad_output</span></code> is ignored and no <code class="docutils literal notranslate"><span class="pre">grad_input</span></code> is computed.</p>
<p>Instead, the gradients for the weights and biases are computed using the variables stored in <code class="docutils literal notranslate"><span class="pre">context</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">grad_weight</span></code>:</p>
<ul>
<li><p>Computed as input neuron activity multiplied by output activity.</p></li>
<li><p>To avoid weight changes scaling linearly with the number of inputs, <strong>we also divide by the number of input neurons</strong>.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">grad_bias</span></code>:</p>
<ul>
<li><p>Computed simply as the output neuron activity, since biases have the same dimension as the output of a layer.</p></li>
<li><p>Biases are enabled here. However, it should be noted that although they are used often in networks using error backpropagation, they are not used as often in Hebbian learning.</p></li>
</ul>
</li>
</ul>
<p><strong>Notes:</strong></p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">backward()</span></code> method expects to return <strong>as many gradient values</strong> as the number of <strong>inputs</strong> passed to the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method (except <code class="docutils literal notranslate"><span class="pre">context</span></code>). It may raise an error if it’s not implemented this way. So this is why, in the example below, <code class="docutils literal notranslate"><span class="pre">backward()</span></code> returns <code class="docutils literal notranslate"><span class="pre">grad_nonlinearity</span></code> and <code class="docutils literal notranslate"><span class="pre">grad_target</span></code>, but they are both <code class="docutils literal notranslate"><span class="pre">None</span></code> values.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">backward()</span></code> method computes the gradients, but <strong>does <em>not</em> apply weight updates</strong>. These are applied when <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> is called. In the <code class="docutils literal notranslate"><span class="pre">BasicOptimizer</span></code>, defined above, the optimizer step optionally applies a weight decay, then subtracts the gradients computed here <strong>multiplied by the learning rate</strong>.</p></li>
<li><p>Standard optimizers, including the <code class="docutils literal notranslate"><span class="pre">BasicOptimizer</span></code>, <strong>expect</strong> to receive error gradients (i.e., values they should <strong>subtract</strong> from the parameters). However, we have computed Hebbian gradients (i.e., values that should be <strong>added</strong> to the parameters). For this reason, the gradients computed are multiplied by <code class="docutils literal notranslate"><span class="pre">-1</span></code> in the <code class="docutils literal notranslate"><span class="pre">backward()</span></code> method before they are returned.</p></li>
</ul>
<hr class="docutils" />
<p>To learn more about <code class="docutils literal notranslate"><span class="pre">torch</span></code>’s autograd function and creating custom functions, see the following <code class="docutils literal notranslate"><span class="pre">torch</span></code> documentation and tutorials:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">A Gentle Introduction to <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></a></p></li>
<li><p>The <strong>Extending <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></strong> section of <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html">Extending PyTorch</a>.</p></li>
<li><p>An <a class="reference external" href="https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html">example</a> of a custom torch autograd function.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">HebbianFunction</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Gradient computing function class for Hebbian learning.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass method for the layer. Computes the output of the layer and</span>
<span class="sd">    stores variables needed for the backward pass.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    - context (torch context): context in which variables can be stored for</span>
<span class="sd">      the backward pass.</span>
<span class="sd">    - input (torch tensor): input to the layer.</span>
<span class="sd">    - weight (torch tensor): layer weights.</span>
<span class="sd">    - bias (torch tensor, optional): layer biases.</span>
<span class="sd">    - nonlinearity (torch functional, optional): nonlinearity for the layer.</span>
<span class="sd">    - target (torch tensor, optional): layer target, if applicable.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - output (torch tensor): layer output.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># compute the output for the layer (linear layer with non-linearity)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">output</span> <span class="o">+=</span> <span class="n">bias</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">nonlinearity</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">nonlinearity</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="c1"># calculate the output to use for the backward pass</span>
    <span class="n">output_for_update</span> <span class="o">=</span> <span class="n">output</span> <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">target</span>

    <span class="c1"># store variables in the context for the backward pass</span>
    <span class="n">context</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">output_for_update</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span>

  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">grad_output</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Backward pass method for the layer. Computes and returns the gradients for</span>
<span class="sd">    all variables passed to forward (returning None if not applicable).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    - context (torch context): context in which variables can be stored for</span>
<span class="sd">      the backward pass.</span>
<span class="sd">    - input (torch tensor): input to the layer.</span>
<span class="sd">    - weight (torch tensor): layer weights.</span>
<span class="sd">    - bias (torch tensor, optional): layer biases.</span>
<span class="sd">    - nonlinearity (torch functional, optional): nonlinearity for the layer.</span>
<span class="sd">    - target (torch tensor, optional): layer target, if applicable.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - grad_input (None): gradients for the input (None, since gradients are not</span>
<span class="sd">      backpropagated in Hebbian learning).</span>
<span class="sd">    - grad_weight (torch tensor): gradients for the weights.</span>
<span class="sd">    - grad_bias (torch tensor or None): gradients for the biases, if they aren&#39;t</span>
<span class="sd">      None.</span>
<span class="sd">    - grad_nonlinearity (None): gradients for the nonlinearity (None, since</span>
<span class="sd">      gradients do not apply to the non-linearities).</span>
<span class="sd">    - grad_target (None): gradients for the targets (None, since</span>
<span class="sd">      gradients do not apply to the targets).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">output_for_update</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">saved_tensors</span>
    <span class="n">grad_input</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">grad_weight</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">grad_bias</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">grad_nonlinearity</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">grad_target</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">input_needs_grad</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">input_needs_grad</span><span class="p">:</span>
      <span class="k">pass</span>

    <span class="n">weight_needs_grad</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">weight_needs_grad</span><span class="p">:</span>
      <span class="n">grad_weight</span> <span class="o">=</span> <span class="n">output_for_update</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
      <span class="n">grad_weight</span> <span class="o">=</span> <span class="n">grad_weight</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># average across batch</span>

      <span class="c1"># center around 0</span>
      <span class="n">grad_weight</span> <span class="o">=</span> <span class="n">grad_weight</span> <span class="o">-</span> <span class="n">grad_weight</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># center around 0</span>

      <span class="c1">## or apply Oja&#39;s rule (not compatible with clamping outputs to the targets!)</span>
      <span class="c1"># oja_subtract = output_for_update.pow(2).mm(grad_weight).mean(axis=0)</span>
      <span class="c1"># grad_weight = grad_weight - oja_subtract</span>

      <span class="c1"># take the negative, as the gradient will be subtracted</span>
      <span class="n">grad_weight</span> <span class="o">=</span> <span class="o">-</span><span class="n">grad_weight</span>

    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">bias_needs_grad</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">bias_needs_grad</span><span class="p">:</span>
        <span class="n">grad_bias</span> <span class="o">=</span> <span class="n">output_for_update</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># average across batch</span>

        <span class="c1"># center around 0</span>
        <span class="n">grad_bias</span> <span class="o">=</span> <span class="n">grad_bias</span> <span class="o">-</span> <span class="n">grad_bias</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1">## or apply an adaptation of Oja&#39;s rule for biases</span>
        <span class="c1">## (not compatible with clamping outputs to the targets!)</span>
        <span class="c1"># oja_subtract = (output_for_update.pow(2) * bias).mean(axis=0)</span>
        <span class="c1"># grad_bias = grad_bias - oja_subtract</span>

        <span class="c1"># take the negative, as the gradient will be subtracted</span>
        <span class="n">grad_bias</span> <span class="o">=</span> <span class="o">-</span><span class="n">grad_bias</span>

    <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_weight</span><span class="p">,</span> <span class="n">grad_bias</span><span class="p">,</span> <span class="n">grad_nonlinearity</span><span class="p">,</span> <span class="n">grad_target</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="defining-a-hebbianmultilayerperceptron-class">
<h4>4.2.4 Defining a <code class="docutils literal notranslate"><span class="pre">HebbianMultiLayerPerceptron</span></code> class<a class="headerlink" href="#defining-a-hebbianmultilayerperceptron-class" title="Permalink to this heading">#</a></h4>
<p>Lastly, we provide a <code class="docutils literal notranslate"><span class="pre">HebbianMultiLayerPerceptron</span></code> class. This class inherits from <code class="docutils literal notranslate"><span class="pre">MultiLayerPerceptron</span></code>, but implements its own <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method. This <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method uses <code class="docutils literal notranslate"><span class="pre">HebbianFunction()</span></code> to send inputs through each layer of the network and its activation function, ensuring that gradients are computed correctly for Hebbian learning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">HebbianMultiLayerPerceptron</span><span class="p">(</span><span class="n">MultiLayerPerceptron</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Hebbian multilayer perceptron with one hidden layer.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">clamp_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes a Hebbian multilayer perceptron object</span>

<span class="sd">    Arguments:</span>
<span class="sd">    - clamp_output (bool, optional): if True, outputs are clamped to targets,</span>
<span class="sd">      if available, when computing weight updates.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">clamp_output</span> <span class="o">=</span> <span class="n">clamp_output</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runs a forward pass through the network.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    - X (torch.Tensor): Batch of input images.</span>
<span class="sd">    - y (torch.Tensor, optional): Batch of targets, stored for the backward</span>
<span class="sd">      pass to compute the gradients for the last layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - y_pred (torch.Tensor): Predicted targets.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">h</span> <span class="o">=</span> <span class="n">HebbianFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_inputs</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin1</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin1</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># if targets are provided, they can be used instead of the last layer&#39;s</span>
    <span class="c1"># output to train the last layer.</span>
    <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">clamp_output</span><span class="p">:</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
          <span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_outputs</span>
          <span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">HebbianFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="n">h</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin2</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin2</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">,</span>
        <span class="n">targets</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">y_pred</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="training-a-model-with-hebbian-learning">
<h3>4.3 Training a model with Hebbian learning<a class="headerlink" href="#training-a-model-with-hebbian-learning" title="Permalink to this heading">#</a></h3>
<section id="simplifying-the-task-to-a-2-class-task">
<h4>4.3.1 Simplifying the task to a 2-class task<a class="headerlink" href="#simplifying-the-task-to-a-2-class-task" title="Permalink to this heading">#</a></h4>
<p>When implementing biologically plausible learning rules, one often faces a <strong>performance trade-off</strong>. This is because it is often harder for a network to learn without the precise error gradients that error backpropagation offers. More sophisticated designs can really enhance performance. However, since we are starting with the basics, we will instead start by <strong>simplifying</strong> the task from a 10-class task to a 2-class task.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown The following function returns a dataset restricted to specific classes:</span>

<span class="c1">#@markdown `restrict_classes(dataset)`: Keeps specified classes in a dataset.</span>

<span class="k">def</span> <span class="nf">restrict_classes</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="n">keep</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Removes or keeps specified classes in a dataset.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - dataset (torch dataset or subset): Dataset with class targets.</span>
<span class="sd">  - classes (list): List of classes to keep or remove.</span>
<span class="sd">  - keep (bool): If True, the classes specified are kept. If False, they are</span>
<span class="sd">  removed.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - new_dataset (torch dataset or subset): Datset restricted as specified.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="s2">&quot;dataset&quot;</span><span class="p">):</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">dataset</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">targets</span>

  <span class="n">specified_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">keep</span><span class="p">:</span>
    <span class="n">retain_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">specified_idxs</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">retain_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="o">~</span><span class="n">specified_idxs</span><span class="p">]</span>

  <span class="n">new_dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Subset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">retain_indices</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">new_dataset</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_set_2_classes</span> <span class="o">=</span> <span class="n">restrict_classes</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">valid_set_2_classes</span> <span class="o">=</span> <span class="n">restrict_classes</span><span class="p">(</span><span class="n">valid_set</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">plot_class_distribution</span><span class="p">(</span><span class="n">train_set_2_classes</span><span class="p">,</span> <span class="n">valid_set_2_classes</span><span class="p">)</span>

<span class="n">train_loader_2cls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set_2_classes</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">valid_loader_2cls</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_set_2_classes</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/fd9f241bf39f69b9c99044e58cd68953964ed0aa71359f890c9e2341e1c08e91.png" src="../../_images/fd9f241bf39f69b9c99044e58cd68953964ed0aa71359f890c9e2341e1c08e91.png" />
</div>
</div>
<p>The number of examples for each class in the training and validation sets are shown. Dashed lines show what counts would be expected if there were the same number of examples in each class.</p>
</section>
<section id="training-on-a-2-class-task">
<h4>4.3.2 Training on a 2-class task<a class="headerlink" href="#training-on-a-2-class-task" title="Permalink to this heading">#</a></h4>
<p>We’ll test the Hebbian learning model’s performance with <code class="docutils literal notranslate"><span class="pre">10</span></code> epochs of training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">HEBB_LR</span> <span class="o">=</span> <span class="mf">1e-4</span> <span class="c1"># lower, since Hebbian gradients are much bigger than backprop gradients</span>

<span class="n">HebbianMLP_2cls</span> <span class="o">=</span> <span class="n">HebbianMultiLayerPerceptron</span><span class="p">(</span>
    <span class="n">num_hidden</span><span class="o">=</span><span class="n">NUM_HIDDEN</span><span class="p">,</span>
    <span class="n">num_outputs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">clamp_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">Hebb_optimizer_2cls</span> <span class="o">=</span> <span class="n">BasicOptimizer</span><span class="p">(</span><span class="n">HebbianMLP_2cls</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">HEBB_LR</span><span class="p">)</span>

<span class="n">Hebb_results_dict_2cls</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">HebbianMLP_2cls</span><span class="p">,</span>
    <span class="n">train_loader_2cls</span><span class="p">,</span>
    <span class="n">valid_loader_2cls</span><span class="p">,</span>
    <span class="n">Hebb_optimizer_2cls</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span>
    <span class="p">);</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">Hebb_results_dict_2cls</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot_scores_per_class</span><span class="p">(</span><span class="n">Hebb_results_dict_2cls</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot_examples</span><span class="p">(</span><span class="n">valid_loader_2cls</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">MLP</span><span class="o">=</span><span class="n">HebbianMLP_2cls</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot_weights</span><span class="p">(</span><span class="n">HebbianMLP_2cls</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/2f7ec2d6d0933ce2328dc148e324a5b8e3314d92110bdd4bf254e069bdca9945.png" src="../../_images/2f7ec2d6d0933ce2328dc148e324a5b8e3314d92110bdd4bf254e069bdca9945.png" />
<img alt="../../_images/51da7ce57e0f2277f7ea35463d4339cbea05c6e2880b1552bbd1d0f5235025f0.png" src="../../_images/51da7ce57e0f2277f7ea35463d4339cbea05c6e2880b1552bbd1d0f5235025f0.png" />
<img alt="../../_images/ebbd94f6b2aca0d5db7e1dfdba8970ecf710258730e07f5cce865ed4e38bef98.png" src="../../_images/ebbd94f6b2aca0d5db7e1dfdba8970ecf710258730e07f5cce865ed4e38bef98.png" />
<img alt="../../_images/514235864a0211a6e3641f86c7f620ed7119109f3ac71610803747d4d04faeb6.png" src="../../_images/514235864a0211a6e3641f86c7f620ed7119109f3ac71610803747d4d04faeb6.png" />
</div>
</div>
<p>Try running the previous cell <strong>several times</strong> to see how often the model succeeds in learning this simple classification task.</p>
<p>❓ <strong>Why does this network struggle to learn this simple classification task?</strong></p>
</section>
<section id="training-with-targets">
<h4>4.3.3 Training with targets<a class="headerlink" href="#training-with-targets" title="Permalink to this heading">#</a></h4>
<p>As you may have notice, the classification <strong>targets</strong> do not actually appear in the basic Hebbian learning rule. How, then, can the network learn to perform a supervised task?</p>
<p>To allow a network trained with Hebbian learning to learn a supervised task, one can take the approach of <strong>clamping the outputs to the targets</strong>. In other words, we can update the final layer’s weights using the targets <span class="math notranslate nohighlight">\(t_{j}\)</span> instead of the final layer’s activations <span class="math notranslate nohighlight">\(a_{j}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">HebbianMLP_2cls</span> <span class="o">=</span> <span class="n">HebbianMultiLayerPerceptron</span><span class="p">(</span>
    <span class="n">num_hidden</span><span class="o">=</span><span class="n">NUM_HIDDEN</span><span class="p">,</span>
    <span class="n">num_outputs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">clamp_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># clamp output to targets</span>
<span class="p">)</span>

<span class="n">Hebb_optimizer_2cls</span> <span class="o">=</span> <span class="n">BasicOptimizer</span><span class="p">(</span><span class="n">HebbianMLP_2cls</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">HEBB_LR</span><span class="p">)</span>

<span class="n">Hebb_results_dict_2cls</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">HebbianMLP_2cls</span><span class="p">,</span>
    <span class="n">train_loader_2cls</span><span class="p">,</span>
    <span class="n">valid_loader_2cls</span><span class="p">,</span>
    <span class="n">Hebb_optimizer_2cls</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span>
    <span class="p">);</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">Hebb_results_dict_2cls</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot_scores_per_class</span><span class="p">(</span><span class="n">Hebb_results_dict_2cls</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot_examples</span><span class="p">(</span><span class="n">valid_loader_2cls</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">MLP</span><span class="o">=</span><span class="n">HebbianMLP_2cls</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot_weights</span><span class="p">(</span><span class="n">HebbianMLP_2cls</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/7e75cfd3cd3e8047660ae3a9211194089a8aa0600d07e4cc8492b2795a25dcd1.png" src="../../_images/7e75cfd3cd3e8047660ae3a9211194089a8aa0600d07e4cc8492b2795a25dcd1.png" />
<img alt="../../_images/51da7ce57e0f2277f7ea35463d4339cbea05c6e2880b1552bbd1d0f5235025f0.png" src="../../_images/51da7ce57e0f2277f7ea35463d4339cbea05c6e2880b1552bbd1d0f5235025f0.png" />
<img alt="../../_images/d05a0f7bf00a1cfd9086d6554e3c4e7a6574c8f062233b0a964fe3041948d3ab.png" src="../../_images/d05a0f7bf00a1cfd9086d6554e3c4e7a6574c8f062233b0a964fe3041948d3ab.png" />
<img alt="../../_images/19886af2cf0cb43d1648a457edcd1d3d2cddb8245b8a18703c00a97563cf7d5b.png" src="../../_images/19886af2cf0cb43d1648a457edcd1d3d2cddb8245b8a18703c00a97563cf7d5b.png" />
</div>
</div>
<p>Try running the previous cell <strong>several times</strong>. The model should now be more successful at learning this simple classification task.</p>
<p>❓ <strong>Is the model successful every time? If not, what might be contributing to the variability in performance?</strong><br />
❓ <strong>Going futher: How does changing the training hyperparameters (learning rate and number of epochs) affect network learning?</strong></p>
<p><strong>Note:</strong> The clamped outputs setting <em>cannot</em> be used with Oja’s rule (i.e., one of the Hebbian learning modifications used to prevent runaway weight increases). This is because the values subtracted when using Oja’s rule would be calculated on the target outputs instead of the actual outputs, and these values would end up being too big.</p>
</section>
<section id="increasing-task-difficulty">
<h4>4.3.4 Increasing task difficulty<a class="headerlink" href="#increasing-task-difficulty" title="Permalink to this heading">#</a></h4>
<p>Next, let’s see what happens when we <strong>increase the number of classes</strong> in the task. The following function handles the entire initialization and training process:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown `train_model_extended`: Initializes model and optimizer, restricts datasets to</span>
<span class="c1">#@markdown specified classes, trains model. Returns trained model and results dictionary.</span>

<span class="k">def</span> <span class="nf">train_model_extended</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;backprop&quot;</span><span class="p">,</span> <span class="n">keep_num_classes</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>
                         <span class="n">num_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">partial_backprop</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">num_hidden</span><span class="o">=</span><span class="n">NUM_HIDDEN</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">BIAS</span><span class="p">,</span>
                         <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">plot_distribution</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Initializes model and optimizer, restricts datasets to specified classes and</span>
<span class="sd">  trains the model. Returns the trained model and results dictionary.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - model_type (str, optional): model to initialize (&quot;backprop&quot; or &quot;Hebbian&quot;)</span>
<span class="sd">  - keep_num_classes (str or int, optional): number of classes to keep (from 0)</span>
<span class="sd">  - lr (float or list, optional): learning rate for both or each layer</span>
<span class="sd">  - num_epochs (int, optional): number of epochs to train model.</span>
<span class="sd">  - partial_backprop (bool, optional): if True, backprop is used to train the</span>
<span class="sd">    final Hebbian learning model.</span>
<span class="sd">  - num_hidden (int, optional): number of hidden units in the hidden layer</span>
<span class="sd">  - bias (bool, optional): if True, each linear layer will have biases in</span>
<span class="sd">      addition to weights.</span>
<span class="sd">  - batch_size (int, optional): batch size for dataloaders.</span>
<span class="sd">  - plot_distribution (bool, optional): if True, dataset class distributions</span>
<span class="sd">    are plotted.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - MLP (torch module): Model</span>
<span class="sd">  - results_dict (dict): Dictionary storing results across epochs on training</span>
<span class="sd">    and validation data.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keep_num_classes</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">keep_num_classes</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span><span class="p">:</span>
      <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
      <span class="n">use_train_set</span> <span class="o">=</span> <span class="n">train_set</span>
      <span class="n">use_valid_set</span> <span class="o">=</span> <span class="n">valid_set</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;If &#39;keep_classes&#39; is a string, it should be &#39;all&#39;.&quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">keep_num_classes</span><span class="p">)</span>
    <span class="n">use_train_set</span> <span class="o">=</span> <span class="n">restrict_classes</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">keep_num_classes</span><span class="p">))</span>
    <span class="n">use_valid_set</span> <span class="o">=</span> <span class="n">restrict_classes</span><span class="p">(</span><span class="n">valid_set</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">keep_num_classes</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">plot_distribution</span><span class="p">:</span>
    <span class="n">plot_class_distribution</span><span class="p">(</span><span class="n">use_train_set</span><span class="p">,</span> <span class="n">use_valid_set</span><span class="p">)</span>

  <span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
      <span class="n">use_train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
      <span class="p">)</span>
  <span class="n">valid_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
      <span class="n">use_valid_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

  <span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s2">&quot;num_hidden&quot;</span><span class="p">:</span> <span class="n">num_hidden</span><span class="p">,</span>
      <span class="s2">&quot;num_outputs&quot;</span><span class="p">:</span> <span class="n">num_classes</span><span class="p">,</span>
      <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="n">bias</span><span class="p">,</span>
  <span class="p">}</span>

  <span class="k">if</span> <span class="n">model_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;backprop&quot;</span><span class="p">:</span>
    <span class="n">Model</span> <span class="o">=</span> <span class="n">MultiLayerPerceptron</span>
  <span class="k">elif</span> <span class="n">model_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;hebbian&quot;</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">partial_backprop</span><span class="p">:</span>
      <span class="n">Model</span> <span class="o">=</span> <span class="n">HebbianBackpropMultiLayerPerceptron</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">Model</span> <span class="o">=</span> <span class="n">HebbianMultiLayerPerceptron</span>
      <span class="n">model_params</span><span class="p">[</span><span class="s2">&quot;clamp_output&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Got </span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s2"> model type, but expected &#39;backprop&#39; or &#39;hebbian&#39;.&quot;</span>
        <span class="p">)</span>

  <span class="n">MLP</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="o">**</span><span class="n">model_params</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;If &#39;lr&#39; is a list, it must be of length 2.&quot;</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">BasicOptimizer</span><span class="p">([</span>
        <span class="p">{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">MLP</span><span class="o">.</span><span class="n">lin1</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="p">[</span><span class="mi">0</span><span class="p">]},</span>
        <span class="p">{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">MLP</span><span class="o">.</span><span class="n">lin2</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="p">[</span><span class="mi">1</span><span class="p">]},</span>
    <span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">BasicOptimizer</span><span class="p">(</span><span class="n">MLP</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>


  <span class="n">results_dict</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
      <span class="n">MLP</span><span class="p">,</span>
      <span class="n">train_loader</span><span class="p">,</span>
      <span class="n">valid_loader</span><span class="p">,</span>
      <span class="n">optimizer</span><span class="p">,</span>
      <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span>
      <span class="p">)</span>

  <span class="k">return</span> <span class="n">MLP</span><span class="p">,</span> <span class="n">results_dict</span>
</pre></div>
</div>
</div>
</div>
<p>First, let’s train a model on this task using error backpropagation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MLP_3cls</span><span class="p">,</span> <span class="n">results_dict_3cls</span> <span class="o">=</span> <span class="n">train_model_extended</span><span class="p">(</span>
    <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;backprop&quot;</span><span class="p">,</span>
    <span class="n">keep_num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>
    <span class="n">plot_distribution</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">results_dict_3cls</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plot_scores_per_class</span><span class="p">(</span><span class="n">results_dict_3cls</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plot_weights</span><span class="p">(</span><span class="n">MLP_3cls</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/dae0ab7c37d7b282a377f98ca25b9daa8faddb6e3fc016169092114af0516a62.png" src="../../_images/dae0ab7c37d7b282a377f98ca25b9daa8faddb6e3fc016169092114af0516a62.png" />
<img alt="../../_images/227122287522fdd096083e75680393de4a7b3d71caf94a56bf06b114d87f2cbf.png" src="../../_images/227122287522fdd096083e75680393de4a7b3d71caf94a56bf06b114d87f2cbf.png" />
<img alt="../../_images/57a264c4e3ac7613d439c3a30b7080c483fc6062889f73cc12d915551fbb1bad.png" src="../../_images/57a264c4e3ac7613d439c3a30b7080c483fc6062889f73cc12d915551fbb1bad.png" />
<img alt="../../_images/6cfe7eb671fa12ad1c1312b1ff01e883cd1693292b8b43982aea7265ad27382f.png" src="../../_images/6cfe7eb671fa12ad1c1312b1ff01e883cd1693292b8b43982aea7265ad27382f.png" />
</div>
</div>
<p>Now, let’s try training a model with Hebbian learning (and outputs clamped to targets). Since the task is harder, we’ll increase the number of training epochs to <code class="docutils literal notranslate"><span class="pre">15</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">HebbianMLP_3cls</span><span class="p">,</span> <span class="n">Hebbian_results_dict_3cls</span> <span class="o">=</span> <span class="n">train_model_extended</span><span class="p">(</span>
    <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;hebbian&quot;</span><span class="p">,</span>
    <span class="n">keep_num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="n">HEBB_LR</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">Hebbian_results_dict_3cls</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plot_scores_per_class</span><span class="p">(</span><span class="n">Hebbian_results_dict_3cls</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plot_weights</span><span class="p">(</span><span class="n">HebbianMLP_3cls</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/c870014ad7add12b5958f4f31059bd446bacdfbfa022eb50fbe1e0b576ae32e7.png" src="../../_images/c870014ad7add12b5958f4f31059bd446bacdfbfa022eb50fbe1e0b576ae32e7.png" />
<img alt="../../_images/746771807d9aaaaf059cf8f8a90c9291d27e19d3dfc31eb4d434ea0efe648648.png" src="../../_images/746771807d9aaaaf059cf8f8a90c9291d27e19d3dfc31eb4d434ea0efe648648.png" />
<img alt="../../_images/dd786452bbb30ddf185a1cf709621d5f5ea5ad259f0d4ab1bbde28dea5aeb05d.png" src="../../_images/dd786452bbb30ddf185a1cf709621d5f5ea5ad259f0d4ab1bbde28dea5aeb05d.png" />
</div>
</div>
<p>Try running the previous cell a few times to see how often the model is successful.</p>
<p>❓ <strong>How is the model learning in each layer?</strong><br />
❓ <strong>How do the weight updates learned with Hebbian learning compare to those learned with error backpropagation?</strong></p>
<p>We can try using <strong>different learning rates</strong> to encourage more learning in the second layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">HebbianMLP_3cls</span><span class="p">,</span> <span class="n">Hebbian_results_dict_3cls</span> <span class="o">=</span> <span class="n">train_model_extended</span><span class="p">(</span>
    <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;hebbian&quot;</span><span class="p">,</span>
    <span class="n">keep_num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="p">[</span><span class="n">HEBB_LR</span> <span class="o">/</span> <span class="mi">4</span><span class="p">,</span> <span class="n">HEBB_LR</span> <span class="o">*</span> <span class="mi">8</span><span class="p">],</span> <span class="c1"># learning rate for each layer</span>
<span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">Hebbian_results_dict_3cls</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plot_scores_per_class</span><span class="p">(</span><span class="n">Hebbian_results_dict_3cls</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plot_weights</span><span class="p">(</span><span class="n">HebbianMLP_3cls</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9eff1cd5e662018144c191ff1162b643b6ac901cdeff76c7c9f12ff46e95e10a.png" src="../../_images/9eff1cd5e662018144c191ff1162b643b6ac901cdeff76c7c9f12ff46e95e10a.png" />
<img alt="../../_images/287fc1ad6aa05f7cc2d424fe18ec5fe669cad2c3979169c6edb45ed804fd4e96.png" src="../../_images/287fc1ad6aa05f7cc2d424fe18ec5fe669cad2c3979169c6edb45ed804fd4e96.png" />
<img alt="../../_images/07c7efd4f5ee0a3047edd0132c664187ddde94e1c9d93f642b7c9f091c7b6e16.png" src="../../_images/07c7efd4f5ee0a3047edd0132c664187ddde94e1c9d93f642b7c9f091c7b6e16.png" />
</div>
</div>
<p>Performance tends to be highly variable and unstable. At best, the network is able to classify 2 classes, but generally not all 3 classes.</p>
</section>
<section id="combining-hebbian-learning-and-error-backpropagation">
<h4>4.3.5 Combining Hebbian learning and error backpropagation<a class="headerlink" href="#combining-hebbian-learning-and-error-backpropagation" title="Permalink to this heading">#</a></h4>
<p>What happens if we use Hebbian learning for the first layer, but use error backpropagation to train the second layer?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown `HebbianBackpropMultiLayerPerceptron()`: Class combining Hebbian learning and backpropagation.</span>

<span class="k">class</span> <span class="nc">HebbianBackpropMultiLayerPerceptron</span><span class="p">(</span><span class="n">MultiLayerPerceptron</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Hybrid backprop/Hebbian multilayer perceptron with one hidden layer.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runs a forward pass through the network.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    - X (torch.Tensor): Batch of input images.</span>
<span class="sd">    - y (torch.Tensor, optional): Batch of targets, not used here.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - y_pred (torch.Tensor): Predicted targets.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Hebbian layer</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">HebbianFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_inputs</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin1</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin1</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># backprop layer</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lin2</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">y_pred</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">HybridMLP</span><span class="p">,</span> <span class="n">Hybrid_results_dict</span> <span class="o">=</span> <span class="n">train_model_extended</span><span class="p">(</span>
    <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;hebbian&quot;</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">partial_backprop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># backprop on the final layer</span>
    <span class="n">lr</span><span class="o">=</span><span class="p">[</span><span class="n">HEBB_LR</span> <span class="o">/</span> <span class="mi">5</span><span class="p">,</span> <span class="n">LR</span><span class="p">],</span> <span class="c1"># learning rates for each layer</span>
<span class="p">)</span>

<span class="n">plot_results</span><span class="p">(</span><span class="n">Hybrid_results_dict</span><span class="p">)</span>
<span class="n">plot_scores_per_class</span><span class="p">(</span><span class="n">Hybrid_results_dict</span><span class="p">)</span>
<span class="n">plot_weights</span><span class="p">(</span><span class="n">HybridMLP</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/d89f2ef9c201ec30ee481c06124a38d0fe8b48d45d199fab640e9f3574529616.png" src="../../_images/d89f2ef9c201ec30ee481c06124a38d0fe8b48d45d199fab640e9f3574529616.png" />
<img alt="../../_images/13d543bd902ebce670200012be4e30fcb77e03c2cd4e5051bcf1d2964aec7e65.png" src="../../_images/13d543bd902ebce670200012be4e30fcb77e03c2cd4e5051bcf1d2964aec7e65.png" />
<img alt="../../_images/3f1137cd619e83f71bf9712f6725484b0756d2e85866c6ac2977de96dee7619a.png" src="../../_images/3f1137cd619e83f71bf9712f6725484b0756d2e85866c6ac2977de96dee7619a.png" />
</div>
</div>
<p>Using Hebbian learning and error backpropagation allows us to achieve above chance performance on the the full MNIST classification task, though performance is still much lower than when using error backpropagation on its own.</p>
<p>❓ <strong>What are some of the properties of Hebbian learning that might explain its weaker performance on this task when compared to error backpropagation?</strong><br />
❓ <strong>Going further: Are there tasks that Hebbian learning might be better at than error backpropagation?</strong></p>
</section>
</section>
<section id="section-4-4-computing-the-variance-and-bias-of-a-model-s-gradients">
<h3>Section 4.4. Computing the variance and bias of a model’s gradients.<a class="headerlink" href="#section-4-4-computing-the-variance-and-bias-of-a-model-s-gradients" title="Permalink to this heading">#</a></h3>
<p>To better understand <em>how</em> a model trained with a biologically plausible learning rule (e.g., Hebbian learning) learns, it can be useful to compare its learning to error backpropagation. Specifically, we can compare the gradients computed with both learning rules to one another.</p>
</section>
<section id="variance">
<h3>Variance<a class="headerlink" href="#variance" title="Permalink to this heading">#</a></h3>
<p>One property we can compute is the variance. The variance tells us <strong>how consistent</strong> are the gradients obtained for each weight <strong>across examples</strong> when computed with one learning rule compared to the other. We might expect a good learning rule to be more consistent, and therefore have <strong>lower variance</strong> in its gradients (<em>left</em> column in the bullseye image).</p>
<p>However, it would be unfair to compare the variance of small gradients (like those computed with error backpropagation) with the variance of large gradients (like those computed with Hebbian learning). So, we will estimate variance in the gradients using a <strong>scale-invariant</strong> measure: the <strong>signal-to-noise ratio (SNR)</strong>. Note that <strong>high SNR</strong> corresponds to <strong>low variance</strong>, and vice versa.</p>
</section>
<section id="bias">
<h3>Bias<a class="headerlink" href="#bias" title="Permalink to this heading">#</a></h3>
<p>Another property we can compute is how biased gradients computed with a specific learning rule are with respect to <strong>ideal gradients</strong>. Now, since the ideal gradients for learning a task are generally unknown, we need to estimate them. We can do so using error backpropagation, as it is the best algorithm we know for learning a task along the gradient of its error. A good learning rule would then be expected to have <strong>low bias</strong> in its gradients with respect to error backpropagation gradients.</p>
<p>As with the variance, our bias estimate should be <strong>scale-invariant</strong>, so we will estimate it using the Cosine similarity. Note that <strong>high Cosine similarity</strong> with error backpropagation gradients corresponds to <strong>low bias</strong>, and vice versa.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown Bias and variance schematic</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/BiasVariance.jpeg?raw=true&quot;</span>

<span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">url</span> <span class="o">=</span> <span class="n">url</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">400</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><img src="https://github.com/neuromatch/NeuroAI_Course/blob/main/projects/project-notebooks/static/BiasVariance.jpeg?raw=true" height="400"/></div></div>
</div>
<p>The bullseye image illustrates the differences and interplay between bias and variance in a variable. In the <strong>left column</strong>, the variable being measured shows <strong>low variance</strong>, as the green dots are densely concentrated. In the <strong>right column</strong>, the dots are more dispersed, reflecting a higher variance.</p>
<p>Although the examples in each column show the same variance, they show different biases. In examples in the <strong>top row</strong>, the variable being measured has a <strong>low bias</strong> with respect to the bullseye, as the dots are centered on the bullseye. In contrast, in the <strong>bottom row</strong>, the variable being measured has a <strong>high bias</strong> a variable with low bias with respect to the bullseye, as the dots are off-center with respect to the bullseye.</p>
<section id="estimating-gradient-variance-using-snr">
<h4>4.4.1 Estimating gradient variance using SNR<a class="headerlink" href="#estimating-gradient-variance-using-snr" title="Permalink to this heading">#</a></h4>
<p>The following functions measure and plot the SNR of the gradients:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown `compute_gradient_SNR(MLP, dataset)`: Passes a dataset through a model</span>
<span class="c1">#@markdown and computes the SNR of the gradients computed by the model for each example.</span>
<span class="c1">#@markdown Returns a dictionary containing the gradient SNRs for each layer of a model.</span>

<span class="k">def</span> <span class="nf">compute_gradient_SNR</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Computes gradient SNRs for a model given a dataset.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - MLP (torch model): Model for which to compute gradient SNRs.</span>
<span class="sd">  - dataset (torch dataset): Dataset with which to compute gradient SNRs.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - SNR_dict (dict): Dictionary compiling gradient SNRs for each parameter</span>
<span class="sd">    (i.e., the weights and/or biases of each layer).</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">MLP</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

  <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

  <span class="n">gradients</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="nb">list</span><span class="p">()</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">MLP</span><span class="o">.</span><span class="n">list_parameters</span><span class="p">()}</span>

  <span class="c1"># initialize a loader with a batch size of 1</span>
  <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

  <span class="c1"># collect gradients computed on the dataset of data</span>
  <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">MLP</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># zero grad before</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">MLP</span><span class="o">.</span><span class="n">gather_gradient_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="n">gradients</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="n">MLP</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># zero grad after, since no optimzer step is taken</span>

  <span class="c1"># aggregate the gradients</span>
  <span class="n">SNR_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="nb">list</span><span class="p">()</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">MLP</span><span class="o">.</span><span class="n">list_parameters</span><span class="p">()}</span>
  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">gradients</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="n">SNR_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_SNR</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">SNR_dict</span>


<span class="k">def</span> <span class="nf">compute_SNR</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the average SNR for of data across the first axis.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    - data (torch Tensor): items x gradients</span>
<span class="sd">    - epsilon (float, optional): value added to the denominator to avoid</span>
<span class="sd">      division by zero.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - avg_SNR (float): average SNR across data items</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">absolute_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">SNR_by_item</span> <span class="o">=</span> <span class="n">absolute_mean</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">avg_SNR</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">SNR_by_item</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">avg_SNR</span>

<span class="c1">#@markdown `plot_gradient_SNRs(SNR_dict)`: Plots gradient SNRs collected in</span>
<span class="c1">#@markdown a dictionary.</span>
<span class="k">def</span> <span class="nf">plot_gradient_SNRs</span><span class="p">(</span><span class="n">SNR_dict</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Plot gradient SNRs for various learning rules.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - SNR_dict (dict): Gradient SNRs for each learning rule.</span>
<span class="sd">  - width (float, optional): Width of the bars.</span>
<span class="sd">  - ax (plt subplot, optional): Axis on which to plot gradient SNRs. If None, a</span>
<span class="sd">    new axis will be created.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - ax (plt subplot): Axis on which gradient SNRs were plotted.  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">wid</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">SNR_dict</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.5</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">wid</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

  <span class="n">xlabels</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="n">SNR_means</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="n">SNR_sems</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="n">SNRs_scatter</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="p">(</span><span class="n">model_type</span><span class="p">,</span> <span class="n">SNRs</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">SNR_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">xlabels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_type</span><span class="p">)</span>
    <span class="n">color</span> <span class="o">=</span> <span class="n">get_plotting_color</span><span class="p">(</span><span class="n">model_idx</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span>
        <span class="n">m</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">SNRs</span><span class="p">),</span> <span class="n">yerr</span><span class="o">=</span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">sem</span><span class="p">(</span><span class="n">SNRs</span><span class="p">),</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">capsize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span>
        <span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">30</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">SNRs</span><span class="p">))]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">m</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">SNRs</span><span class="p">),</span> <span class="n">SNRs</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xlabels</span><span class="p">))</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">x_pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">+</span> <span class="n">width</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.3</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">x_pad</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">x_pad</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xlabels</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Learning rule&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;SNR&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;SNR of the gradients&quot;</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<p>For the error backpropagation model and the Hebbian learning model, we compute the SNR <strong>before training the model</strong>, using the validation set. This allows us to evaluate the gradients a learning rule proposes for an untrained model. Notably, we pass one example at a time through the model, to obtain gradients for each example.</p>
<p><strong>Note:</strong> Since we obtain a gradient SNR for each layer of the model, here we plot the gradient SNR averaged across layers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SNR_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;backprop&quot;</span><span class="p">,</span> <span class="s2">&quot;Hebbian&quot;</span><span class="p">]:</span>
  <span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s2">&quot;num_hidden&quot;</span><span class="p">:</span> <span class="n">NUM_HIDDEN</span><span class="p">,</span>
      <span class="s2">&quot;activation_type&quot;</span><span class="p">:</span> <span class="n">ACTIVATION</span><span class="p">,</span>
      <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="n">BIAS</span><span class="p">,</span>
  <span class="p">}</span>

  <span class="k">if</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;Hebbian&quot;</span><span class="p">:</span>
    <span class="n">model_fct</span> <span class="o">=</span> <span class="n">HebbianMultiLayerPerceptron</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">model_fct</span> <span class="o">=</span> <span class="n">MultiLayerPerceptron</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">model_fct</span><span class="p">(</span><span class="o">**</span><span class="n">model_params</span><span class="p">)</span>

  <span class="n">model_SNR_dict</span> <span class="o">=</span> <span class="n">compute_gradient_SNR</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">valid_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>

  <span class="n">SNR_dict</span><span class="p">[</span><span class="n">model_type</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">SNR</span> <span class="k">for</span> <span class="n">SNR</span> <span class="ow">in</span> <span class="n">model_SNR_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>

<span class="n">plot_gradient_SNRs</span><span class="p">(</span><span class="n">SNR_dict</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/facaa6ff80ca89107b8f9a7cf7853d03248f7bd023b8211eb069512fa82c242c.png" src="../../_images/facaa6ff80ca89107b8f9a7cf7853d03248f7bd023b8211eb069512fa82c242c.png" />
</div>
</div>
<p>The dots in the plots show the SNR <strong>for each layer</strong> (small dot: first layer; larger dot: second layer).</p>
<p>Hebbian learning appears to have produced gradients with a very <strong>high SNR</strong> (and therefore lower variance) in the <strong>first layer</strong> (small dot). In constrast, the <strong>second layer</strong> (big dot) shows a <strong>lower SNR</strong> than error backpropagation.</p>
<p>Notably, this is evaluated on the full 10-class task which Hebbian learning struggles to learn.</p>
<p>❓ <strong>What does the Hebbian learning SNR look like for the 2-class version of the task?</strong><br />
❓ <strong>What might this mean about how Hebbian learning learns?</strong><br />
❓ <strong>Going further: How might this result relate to the Hebbian learning rule’s performance on the classification task?</strong></p>
</section>
<section id="estimating-the-gradient-bias-with-respect-to-error-backpropagation-using-the-cosine-similarity">
<h4>4.4.2 Estimating the gradient bias with respect to error backpropagation using the Cosine similarity.<a class="headerlink" href="#estimating-the-gradient-bias-with-respect-to-error-backpropagation-using-the-cosine-similarity" title="Permalink to this heading">#</a></h4>
<p>The following functions measure and plot the Cosine similarity of the gradients to error backpropagation gradients:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown `train_and_calculate_cosine_sim(MLP, train_loader, valid_loader, optimizer)`:</span>
<span class="c1">#@markdown Trains a model using a specific learning rule, while computing the cosine</span>
<span class="c1">#@markdown similarity of the gradients proposed the learning rule compared to those proposed</span>
<span class="c1">#@markdown by error backpropagation.</span>
<span class="k">def</span> <span class="nf">train_and_calculate_cosine_sim</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span>
                                   <span class="n">num_epochs</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Train model across epochs, calculating the cosine similarity between the</span>
<span class="sd">  gradients proposed by the learning rule it&#39;s trained with, compared to those</span>
<span class="sd">  proposed by error backpropagation.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - MLP (torch model): Model to train.</span>
<span class="sd">  - train_loader (torch dataloader): Dataloader to use to train the model.</span>
<span class="sd">  - valid_loader (torch dataloader): Dataloader to use to validate the model.</span>
<span class="sd">  - optimizer (torch optimizer): Optimizer to use to update the model.</span>
<span class="sd">  - num_epochs (int, optional): Number of epochs to train model.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - cosine_sim (dict): Dictionary storing the cosine similarity between the</span>
<span class="sd">    model&#39;s learning rule and backprop across epochs, computed on the</span>
<span class="sd">    validation data.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

  <span class="n">cosine_sim_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="nb">list</span><span class="p">()</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">MLP</span><span class="o">.</span><span class="n">list_parameters</span><span class="p">()}</span>

  <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)):</span>
    <span class="n">MLP</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
      <span class="n">y_pred</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">e</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">MLP</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">lr_gradients_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="nb">list</span><span class="p">()</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">cosine_sim_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
    <span class="n">backprop_gradients_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="nb">list</span><span class="p">()</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">cosine_sim_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>

    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">valid_loader</span><span class="p">:</span>
      <span class="c1"># collect gradients computed with learning rule</span>
      <span class="n">y_pred</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>

      <span class="n">MLP</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">MLP</span><span class="o">.</span><span class="n">gather_gradient_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">lr_gradients_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
      <span class="n">MLP</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

      <span class="c1"># collect gradients computed with backprop</span>
      <span class="n">y_pred</span> <span class="o">=</span> <span class="n">MLP</span><span class="o">.</span><span class="n">forward_backprop</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>

      <span class="n">MLP</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">MLP</span><span class="o">.</span><span class="n">gather_gradient_dict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">backprop_gradients_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
      <span class="n">MLP</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">cosine_sim_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
      <span class="n">lr_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">lr_gradients_dict</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
      <span class="n">bp_grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">backprop_gradients_dict</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">lr_grad</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;Learning rule computed all 0 gradients for epoch </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. &quot;</span>
          <span class="s2">&quot;Cosine similarity cannot be calculated.&quot;</span>
          <span class="p">)</span>
        <span class="n">epoch_cosine_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
      <span class="k">elif</span> <span class="p">(</span><span class="n">bp_grad</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
          <span class="sa">f</span><span class="s2">&quot;Backprop. rule computed all 0 gradients for epoch </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. &quot;</span>
          <span class="s2">&quot;Cosine similarity cannot be calculated.&quot;</span>
          <span class="p">)</span>
        <span class="n">epoch_cosine_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">epoch_cosine_sim</span> <span class="o">=</span> <span class="n">calculate_cosine_similarity</span><span class="p">(</span><span class="n">lr_grad</span><span class="p">,</span> <span class="n">bp_grad</span><span class="p">)</span>

      <span class="n">cosine_sim_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_cosine_sim</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">cosine_sim_dict</span>


<span class="k">def</span> <span class="nf">calculate_cosine_similarity</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">data2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the cosine similarity between two vectors.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    - data1 (torch Tensor): first vector</span>
<span class="sd">    - data2 (torch Tensor): second vector</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">data1</span> <span class="o">=</span> <span class="n">data1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">data2</span> <span class="o">=</span> <span class="n">data2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">data2</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">data1</span><span class="p">,</span> <span class="n">data1</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">data2</span><span class="p">,</span> <span class="n">data2</span><span class="p">))</span>
      <span class="p">)</span>

    <span class="n">cosine_sim</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="k">return</span> <span class="n">cosine_sim</span>

<span class="c1">#@markdown `plot_gradient_cosine_sims(cosine_sim_dict)`: Plots cosine similarity</span>
<span class="c1">#@markdown of the gradients proposed by a model across learning to those proposed</span>
<span class="c1">#@markdown by error backpropagation.</span>
<span class="k">def</span> <span class="nf">plot_gradient_cosine_sims</span><span class="p">(</span><span class="n">cosine_sim_dict</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">  Plot gradient cosine similarities to error backpropagation for various</span>
<span class="sd">  learning rules.</span>

<span class="sd">  Arguments:</span>
<span class="sd">  - cosine_sim_dict (dict): Gradient cosine similarities for each learning rule.</span>
<span class="sd">  - ax (plt subplot, optional): Axis on which to plot gradient cosine</span>
<span class="sd">    similarities. If None, a new axis will be created.</span>

<span class="sd">  Returns:</span>
<span class="sd">  - ax (plt subplot): Axis on which gradient cosine similarities were plotted.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

  <span class="n">max_num_epochs</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">m</span><span class="p">,</span> <span class="p">(</span><span class="n">model_type</span><span class="p">,</span> <span class="n">cosine_sims</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cosine_sim_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">cosine_sims</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">cosine_sims</span><span class="p">)</span> <span class="c1"># params x epochs</span>
    <span class="n">num_epochs</span> <span class="o">=</span> <span class="n">cosine_sims</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)</span>
    <span class="n">cosine_sim_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">cosine_sims</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">cosine_sim_sems</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">sem</span><span class="p">(</span><span class="n">cosine_sims</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">nan_policy</span><span class="o">=</span><span class="s2">&quot;omit&quot;</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cosine_sim_means</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">model_type</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

    <span class="n">color</span> <span class="o">=</span> <span class="n">get_plotting_color</span><span class="p">(</span><span class="n">model_idx</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">cosine_sim_means</span> <span class="o">-</span> <span class="n">cosine_sim_sems</span><span class="p">,</span>
        <span class="n">cosine_sim_means</span> <span class="o">+</span> <span class="n">cosine_sim_sems</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span>
        <span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param_cosine_sims</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cosine_sims</span><span class="p">):</span>
      <span class="n">s</span> <span class="o">=</span> <span class="mi">20</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">30</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">param_cosine_sims</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

    <span class="n">max_num_epochs</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_num_epochs</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">max_num_epochs</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_num_epochs</span><span class="p">)</span>
    <span class="n">xlabels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">xlabels</span><span class="p">)</span>

  <span class="n">ymin</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">ymin</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">ymin</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ymin</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Cosine similarity&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Cosine similarity to backprop gradients&quot;</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">cosine_sim_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">model_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;backprop&quot;</span><span class="p">,</span> <span class="s2">&quot;Hebbian&quot;</span><span class="p">]:</span>
  <span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s2">&quot;num_hidden&quot;</span><span class="p">:</span> <span class="n">NUM_HIDDEN</span><span class="p">,</span>
      <span class="s2">&quot;activation_type&quot;</span><span class="p">:</span> <span class="n">ACTIVATION</span><span class="p">,</span>
      <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="n">BIAS</span><span class="p">,</span>
  <span class="p">}</span>

  <span class="k">if</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;Hebbian&quot;</span><span class="p">:</span>
    <span class="n">model_fct</span> <span class="o">=</span> <span class="n">HebbianMultiLayerPerceptron</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">HEBB_LR</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">model_fct</span> <span class="o">=</span> <span class="n">MultiLayerPerceptron</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">LR</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">model_fct</span><span class="p">(</span><span class="o">**</span><span class="n">model_params</span><span class="p">)</span>

  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">BasicOptimizer</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Collecting Cosine similarities for </span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s2">-trained model...&quot;</span><span class="p">)</span>
  <span class="n">model_cosine_sim_dict</span> <span class="o">=</span> <span class="n">train_and_calculate_cosine_sim</span><span class="p">(</span>
      <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span>
      <span class="p">)</span>

  <span class="n">cosine_sim_dict</span><span class="p">[</span><span class="n">model_type</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
      <span class="n">cos_sim</span> <span class="k">for</span> <span class="n">cos_sim</span> <span class="ow">in</span> <span class="n">model_cosine_sim_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
      <span class="p">]</span>

<span class="n">plot_gradient_cosine_sims</span><span class="p">(</span><span class="n">cosine_sim_dict</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Collecting Cosine similarities for backprop-trained model...
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Collecting Cosine similarities for Hebbian-trained model...
</pre></div>
</div>
<img alt="../../_images/11e7d04f19d1b4d7fa886677f928ea6bc9d59aa098ec8bb9b4b5fc6b97d0eec5.png" src="../../_images/11e7d04f19d1b4d7fa886677f928ea6bc9d59aa098ec8bb9b4b5fc6b97d0eec5.png" />
</div>
</div>
<p>As expected, gradient updates proposed by error backpropagation necessarily have <strong>no bias</strong> with respect to themselves (cosine similarity near 1.0). In contrast, although gradient updates proposed by Hebbian learning for the <strong>second layer</strong> (big dots) are <strong>well aligned</strong> with error backpropagation updates, the updates proposed for the <strong>first layer</strong> (small dots) are <strong>highly biased</strong> (cosine similarity near 0.0).</p>
<p>❓ <strong>What might this result tell us about how Hebbian learning learns in this task?</strong><br />
❓ <strong>What does the Hebbian learning cosine similarity to error backpropagation look like for the 2-class version of the task?</strong><br />
❓ <strong>Going further: How might this result relate to the Hebbian learning rule’s performance on the classification task?</strong></p>
<p>❓ <strong>Taken together, what do the bias and variance properties of each layer tell us about how Hebbian learning learns in this task?</strong>
❓ <strong>Going futher: How learning rule-specific are the bias and variance properties of the gradients compared to other performance or learning metrics?</strong><br />
❓ <strong>Going futher: How do the bias and variance of the gradients relate to the performance of a learning rule on a task?</strong></p>
</section>
</section>
</section>
<section id="section-5-implementing-additional-learning-rules">
<h2>Section 5. Implementing additional learning rules.<a class="headerlink" href="#section-5-implementing-additional-learning-rules" title="Permalink to this heading">#</a></h2>
<p>In this notebook, we implemented learning in a neural network using <strong>Hebbian learning</strong>, and examined how this learning rule performed under various scenarios. Hebbian learning is only one biologically plausible learning rule among many others. Importantly, many of these other rules are better suited to <strong>supervised learning tasks</strong> like image classification.</p>
<p>Examples of basic biologically plausible learning rules to explore include <strong>node perturbation</strong>, <strong>weight_perturbation</strong>, <strong>feedback alignment</strong>, and the <strong>Kolen Pollack</strong>.</p>
<p>Take a look at Neuromatch’s NeuroAI tutorial for the Microlearning day for implementations of these algorithms using numpy. Then, see whether you can reimplement one or several of them using <strong>custom <code class="docutils literal notranslate"><span class="pre">torch</span></code> autograd functions</strong>, as demonstrated in this notebook.</p>
<p>Implementing certain learning rules may also require making some changes to how the <strong>optimizer step</strong> is performed. To do so, you can adapt the <code class="docutils literal notranslate"><span class="pre">BasicOptimizer()</span></code> or any other <code class="docutils literal notranslate"><span class="pre">torch</span></code> optimizer, as needed.</p>
<p>The following repositories could be very helpful resources, as they implement these learning rules using custom <code class="docutils literal notranslate"><span class="pre">torch</span></code> autograd functions:</p>
<ul class="simple">
<li><p><strong>Feedback alignment:</strong> https://github.com/L0SG/feedback-alignment-pytorch/blob/master/lib/fa_linear.py</p></li>
<li><p><strong>Kolen-Pollack:</strong> https://github.com/limberc/DL-without-Weight-Transport-PyTorch/blob/master/linear.py. Also take a look at <code class="docutils literal notranslate"><span class="pre">main.py</span></code> to see how they adapt the SGD optimizer to produce the correct updates for Kolen Pollack.</p></li>
</ul>
</section>
<section id="section-6-tips-suggestions">
<h2>Section 6. Tips &amp; Suggestions<a class="headerlink" href="#section-6-tips-suggestions" title="Permalink to this heading">#</a></h2>
<p>Here are a few tips that may be helpful as you delve into questions from the project template.</p>
<ul class="simple">
<li><p><strong>Testing whether the metrics are specific to a learning rule:</strong> There are a few ways to assess whether certain performance and learning metrics are specific to a learning rule. Examples include:</p>
<ul>
<li><p><strong>Visualization:</strong> You could plot the metrics or a lower-dimensional version of the metrics to visualize whether the metrics for each learning rule form separate clusters or whether they all mix together.</p></li>
<li><p><strong>Classification:</strong> You could test whether a linear classifier can be trained to correctly predict the learning rule from the metrics.</p></li>
</ul>
</li>
<li><p><strong>Assessing how your models respond to more challenging learning scenarios</strong>: There are several challenging learning scenarios you can implement. Examples include:</p>
<ul>
<li><p><strong>Online learning:</strong> Training with a batch size of one.</p></li>
<li><p><strong>Non-stationary data:</strong> Changing the distribution of the data across learning. Here, the <code class="docutils literal notranslate"><span class="pre">restrict_classes(dataset)</span></code> function may be useful. For example, you could initially train a model on a dataset with no examples from the <code class="docutils literal notranslate"><span class="pre">6</span></code> class, and then introduce examples from the <code class="docutils literal notranslate"><span class="pre">6</span></code> class partway through training to see how this affects learning.</p></li>
</ul>
</li>
</ul>
</section>
<section id="additional-references">
<h2>Additional References<a class="headerlink" href="#additional-references" title="Permalink to this heading">#</a></h2>
<p>Original papers introducing these biologically plausible learning rules:</p>
<ul class="simple">
<li><p>Node perturbation (Andes et al., 1990, IJCNN)</p></li>
<li><p>Weight perturbation (<a class="reference external" href="https://www.pnas.org/doi/abs/10.1073/pnas.88.10.4433">Mazzoni et al., 1991, PNAS</a>)</p></li>
<li><p>Feedback alignment (<a class="reference external" href="https://www.nature.com/articles/ncomms13276">Lillicrap et al., 2016, Nature Communications</a>)</p></li>
<li><p>Kolen-Pollack algorithm (<a class="reference external" href="https://ieeexplore.ieee.org/document/374486">Kolen &amp; Pollack, 1994, ICNN</a>; <a class="reference external" href="https://arxiv.org/abs/1904.05391">Akrout et al., 2019, NeurIPS</a>)</p></li>
</ul>
<section id="we-hope-you-enjoy-working-on-your-project-and-through-the-process-make-some-interesting-discoveries-about-the-challenges-and-potentials-of-biologically-plausible-learning">
<h3>⭐ We hope you enjoy working on your project and, through the process, make some interesting discoveries about the challenges and potentials of biologically plausible learning! ⭐<a class="headerlink" href="#we-hope-you-enjoy-working-on-your-project-and-through-the-process-make-some-interesting-discoveries-about-the-challenges-and-potentials-of-biologically-plausible-learning" title="Permalink to this heading">#</a></h3>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./projects/project-notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

            </article>
            

            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='left-prev' id="prev-link" href="Macrocircuits.html" title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
          <p class="prev-next-subtitle">previous</p>
          <p class="prev-next-title">Macrocircuits: leveraging neural architectural priors and modularity in embodied agents</p>
      </div>
  </a>
  <a class='right-next' id="next-link" href="ComparingNetworks.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title">Comparing networks: characterizing computational similarity in task-trained recurrent neural networks</p>
  </div>
  <i class="fa-solid fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-1-initial-setup">
   Section 1: Initial Setup
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#download-mnist-dataset">
     1.1 Download MNIST dataset
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#explore-the-dataset">
       1.2 Explore the dataset
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-2-training-a-basic-model">
   Section 2: Training a basic model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#defining-a-basic-model">
     2.1 Defining a basic model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initializing-the-model">
     2.2 Initializing the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#defining-and-initializing-an-optimizer">
     2.3 Defining and initializing an optimizer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-a-basic-model">
     2.4 Training a basic model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-3-inspecting-a-model-s-performance">
   Section 3. Inspecting a model’s performance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-and-accuracy-across-learning">
     3.1 Loss and accuracy across learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#final-accuracy-per-class">
     3.2 Final accuracy per class
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classified-example-images">
     3.3 Classified example images
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weights-before-and-after-learning">
     3.4 Weights before and after learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-4-implementing-a-biologically-plausible-learning-rule">
   Section 4. Implementing a biologically plausible learning rule.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hebbian-learning">
     4.1 Hebbian learning
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#preventing-runaway-potentiation">
       4.1.1 Preventing runaway potentiation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementing-learning-rules-in-torch">
     4.2 Implementing learning rules in
     <code class="docutils literal notranslate">
      <span class="pre">
       torch
      </span>
     </code>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-a-custom-autograd-function">
       4.2.1 Defining a custom autograd function
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-the-forward-method">
       4.2.2 Defining the
       <code class="docutils literal notranslate">
        <span class="pre">
         forward()
        </span>
       </code>
       method
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-the-backward-method">
       4.2.3 Defining the
       <code class="docutils literal notranslate">
        <span class="pre">
         backward()
        </span>
       </code>
       method
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#defining-a-hebbianmultilayerperceptron-class">
       4.2.4 Defining a
       <code class="docutils literal notranslate">
        <span class="pre">
         HebbianMultiLayerPerceptron
        </span>
       </code>
       class
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-a-model-with-hebbian-learning">
     4.3 Training a model with Hebbian learning
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#simplifying-the-task-to-a-2-class-task">
       4.3.1 Simplifying the task to a 2-class task
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-on-a-2-class-task">
       4.3.2 Training on a 2-class task
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-with-targets">
       4.3.3 Training with targets
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#increasing-task-difficulty">
       4.3.4 Increasing task difficulty
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#combining-hebbian-learning-and-error-backpropagation">
       4.3.5 Combining Hebbian learning and error backpropagation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#section-4-4-computing-the-variance-and-bias-of-a-model-s-gradients">
     Section 4.4. Computing the variance and bias of a model’s gradients.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variance">
     Variance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias">
     Bias
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#estimating-gradient-variance-using-snr">
       4.4.1 Estimating gradient variance using SNR
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#estimating-the-gradient-bias-with-respect-to-error-backpropagation-using-the-cosine-similarity">
       4.4.2 Estimating the gradient bias with respect to error backpropagation using the Cosine similarity.
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-5-implementing-additional-learning-rules">
   Section 5. Implementing additional learning rules.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#section-6-tips-suggestions">
   Section 6. Tips &amp; Suggestions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additional-references">
   Additional References
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#we-hope-you-enjoy-working-on-your-project-and-through-the-process-make-some-interesting-discoveries-about-the-challenges-and-potentials-of-biologically-plausible-learning">
     ⭐ We hope you enjoy working on your project and, through the process, make some interesting discoveries about the challenges and potentials of biologically plausible learning! ⭐
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Neuromatch
</p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    <p class="last-updated">
Last updated on None.<br>
</p>
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div>
<a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png"></a>
<a href="https://opensource.org/licenses/BSD-3-Clause"><img src="https://camo.githubusercontent.com/9b9ea65d95c9ef878afa1987df65731d47681336/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736561626f726e2e737667"></a>
The contents of this repository are shared under the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
Software elements are additionally licensed under the <a href="https://opensource.org/licenses/BSD-3-Clause">BSD (3-Clause) License</a>.
</div>

</div>
  </div>
  
</div>
          </div>
        </footer>
        

      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  </body>
</html>